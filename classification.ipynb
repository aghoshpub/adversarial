{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load signal, backgound data\n",
    "vbf_events = pd.read_hdf(\"../MC_Prod_v12/vbf_events.hdf\", \"vbf\") #do hdf5!!\n",
    "ggf_events = pd.read_hdf(\"../MC_Prod_v12/ggF_events.hdf\", \"ggF\")\n",
    "qq_events = pd.read_hdf(\"../MC_Prod_v12/qq_all_events.hdf\", \"qq_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbf_events[\"class\"] = 1\n",
    "ggf_events[\"class\"] = 2 # need to reweight ggF better! set to 0 afterwards\n",
    "qq_events[\"class\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60.047187373963304, 6.0885523004705586, 8.7771621167023159)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.047187373963304"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = qq_events.weight_couplings.sum(), vbf_events.weight_couplings.sum(), ggf_events.weight_couplings.sum()\n",
    "print class_weights\n",
    "max(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([vbf_events, ggf_events, qq_events])\n",
    "#data = pd.concat([vbf_events, ggf_events])\n",
    "#print data.isnull().values.any()\n",
    "#data.describe()\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle the events\n",
    "target = data[\"class\"]\n",
    "mass = data[\"m4l_fsr\"]\n",
    "weights = data[\"weight_couplings\"]\n",
    "del data[\"class\"]\n",
    "del data[\"m4l_fsr\"]\n",
    "del data[\"weight_couplings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = mass.apply(np.log)\n",
    "mass_max, mass_min = mass.max(), mass.min()\n",
    "mass = (mass - mass_min)/(mass_max - mass_min) #!!! save max, min values to file\n",
    "#mass.describe()\n",
    "#plt.hist(mass)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweight Events\n",
    "Training: 1000x everything, fraction 1/3 VBF 1/3 ggF 1/3 qq ~~0.5 VBF, 0.25 ggF, 0.25 qq~~\n",
    "\n",
    "Testing: Back to original VBF, ggF, qq sum of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_size = 0.01 #0.75 # 0.1 # !!!\n",
    "X_train, X_test, y_train, y_test, mass_train, mass_test, weights_train, weights_test = \\\n",
    "    train_test_split(data, target, mass, weights, train_size=train_size)\n",
    "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
    "y_train, y_test, mass_train, mass_test, weights_train, weights_test = \\\n",
    " y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "    mass_train.reset_index(drop=True), mass_test.reset_index(drop=True), \\\n",
    "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class_weights_test = weights_test[y_test == 0].sum(), weights_test[y_test == 1].sum(), weights_test[y_test == 2].sum()\n",
    "scale_up = 1000.\n",
    "for i in xrange(3):\n",
    "    weights_train[y_train == i] *= scale_up*max(class_weights)/ class_weights[i]\n",
    "    weights_test[y_test == i] *= class_weights[i]/class_weights_test[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598.37438156054645, 599.9713208444357, 534.63339133752697)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum(), weights_train[y_train == 2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60.047187373963311, 6.0885523004705586, 8.7771621167023177)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_test[y_test == 0].sum(), weights_test[y_test == 1].sum(), weights_test[y_test == 2].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make ggF background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == 2] = 0\n",
    "y_test[y_test == 2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "#@TODO: check other activations in Andreas, Gilles pivot\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "Dx = Dense(32, activation=\"relu\")(inputs)\n",
    "Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "Dx = Dense(1, activation=\"sigmoid\")(Dx)\n",
    "D = Model(input=[inputs], output=[Dx])\n",
    "D.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_train *=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(weights_train ==0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_class_weights(y, smooth_factor=0):\n",
    "    \"\"\"\n",
    "    Returns the weights for each class based on the frequencies of the samples\n",
    "    :param smooth_factor: factor that smooths extremely uneven weights\n",
    "    :param y: list of true labels (the labels must be hashable)\n",
    "    :return: dictionary with the weight for each class\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    counter = Counter(y)\n",
    "\n",
    "    if smooth_factor > 0:\n",
    "        p = max(counter.values()) * smooth_factor\n",
    "        for k in counter.keys():\n",
    "            counter[k] += p\n",
    "\n",
    "    majority = max(counter.values())\n",
    "\n",
    "    return {cls: float(majority) / count for cls, count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=10)\n",
    "#D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=1) #short for testing purposes\n",
    "#D.fit(X_train, y_train, nb_epoch=10) #unweighted training\n",
    "#D.fit(X_train, y_train, nb_epoch=10, class_weight=get_class_weights(y_train)) #Only interclass weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_class_weights(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vbf_events), len(ggf_events), len(qq_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test ==2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = D.predict(X_test)\n",
    "y_pred = y_pred.ravel()\n",
    "roc_auc_score(y_true=y_test, y_score=y_pred, sample_weight=weights_test)\n",
    "#roc_auc_score(y_true=y_test, y_score=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = D.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_pred_test_sig = [weights_train[(y_train ==1) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "#int_pred_test_bkg = [weights_train[(y_train ==0) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "int_pred_test_sig = [weights_test[(y_test ==1) & (y_pred > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg = [weights_test[(y_test ==0) & (y_pred > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),int_pred_test_sig)\n",
    "plt.plot(np.linspace(0,1,num=50),int_pred_test_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import amsasimov\n",
    "vamsasimov = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig,int_pred_test_bkg)]\n",
    "significance = max(vamsasimov)\n",
    "threshold = np.linspace(0,1,num=50)[ np.array(vamsasimov).argmax() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import compare_train_test\n",
    "compare_train_test(y_pred_train, y_train, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mass_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(y_pred[mass_test<mass_test.mean()], weights=weights_test[mass_test<mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred[mass_test>=mass_test.mean()], weights=weights_test[mass_test>=mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "#plt.hist(y_pred[mass_test<mass.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "#plt.hist(y_pred[mass_test>=mass.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "\n",
    "plt.ylim(0, 5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()# @TODO: do sep for signal background, plot mass : full mass dist, vs after cut on bdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signal_low = list (set( np.where(y_test==1)[0] ) & set( np.where(mass_test<mass_test.mean())[0]))\n",
    "signal_high = list (set( np.where(y_test==1)[0] ) & set( np.where(mass_test>=mass_test.mean())[0]))\n",
    "\n",
    "plt.hist(y_pred[signal_low], weights=weights_test[signal_low], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred[signal_high], weights=weights_test[signal_high], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.title(\"Predicted scores for VBF events for low and high mass\")\n",
    "plt.ylim(0, 8)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bkg_low = list (set( np.where(y_test==0)[0] ) & set( np.where(mass_test<mass_test.mean())[0]))\n",
    "bkg_high = list (set( np.where(y_test==0)[0] ) & set( np.where(mass_test>=mass_test.mean())[0]))\n",
    "\n",
    "plt.hist(y_pred[bkg_low], weights=weights_test[bkg_low], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred[bkg_high], weights=weights_test[bkg_high], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.title(\"Predicted scores for background events for low and high mass\")\n",
    "plt.ylim(0, 4)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGevMass(mass):\n",
    "    return np.exp (mass * (mass_max - mass_min) + mass_min)\n",
    "plt.hist(getGevMass(mass_test), weights=weights_test, bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[y_pred >= threshold]), weights=weights_test[y_pred >= threshold], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score > \" + str(threshold))\n",
    "\n",
    "plt.title(\"Mass Distribution (s+b)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((len(mass_test),), dtype=[('mass',np.float64),('weight',np.float64),('NN_score',np.float64) ])\n",
    "temp['mass'] = np.array(getGevMass(mass_test))\n",
    "temp['weight'] = np.array(weights_test)\n",
    "temp['NN_score'] = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from root_numpy import array2tree\n",
    "tree = array2tree(temp)\n",
    "\n",
    "from ROOT import TEfficiency, TH1F\n",
    "bins = 50\n",
    "scoremin = temp['mass'].min()\n",
    "scoremax = temp['mass'].max()\n",
    "hpreselect = TH1F(\"hpreselect\", \"mass distribution before NN\", bins, scoremin, scoremax)\n",
    "hpreselect.Sumw2()\n",
    "hNN = TH1F(\"hNN\", \"mass distribution for NN Score > \" + str(threshold), bins, scoremin, scoremax)\n",
    "hNN.Sumw2()\n",
    "#tree.Project(\"hpreselect\", \"mass\", \"weight\" ) #Tefficiency can;t do weights\n",
    "tree.Project(\"hpreselect\", \"mass\" )\n",
    "#tree.Project(\"hNN\", \"mass\", \"weight*(NN_score>=\" +str(threshold) + \")\" )\n",
    "tree.Project(\"hNN\", \"mass\", \"(NN_score>=\" +str(threshold) + \")\" )\n",
    "\n",
    "print TEfficiency.CheckConsistency(hNN, hpreselect)\n",
    "pEff = TEfficiency(hNN, hpreselect)\n",
    "\n",
    "from ROOT import TCanvas\n",
    "c = TCanvas(\"myCanvasName\",\"The Canvas Title\",800,350)\n",
    "pEff.SetTitle(\"Efficiency: Pre-selection vs Post NN Selection;Mass (GeV) ;#epsilon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pEff.Draw(\"AP\")\n",
    "#ROOT.enableJSVis()\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(getGevMass(mass_test[y_test==1]), weights=weights_test[y_test==1], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[(y_test==1) & (y_pred >= threshold)]), weights=weights_test[(y_test==1) & (y_pred >= threshold)], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(round(threshold,2)))\n",
    "\n",
    "plt.title(\"VBF Events\")\n",
    "plt.xlabel(\"Mass (GeV)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(220,800 )\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(getGevMass(mass_test[y_test==0]), weights=weights_test[y_test==0], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[(y_test==0) & (y_pred >= threshold)]), weights=weights_test[(y_test==0) & (y_pred >= threshold)], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(round(threshold,2)))\n",
    "\n",
    "plt.title(\"Background Like Events (qq, ggF)\")\n",
    "plt.xlim(220,800 )\n",
    "plt.xlabel(\"Mass (GeV)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr = pearsonr(mass_test, y_pred)\n",
    "print \"Unweighted correlation of all test events with mass is\", corr\n",
    "\n",
    "corr = pearsonr(mass_test[y_test ==1], y_pred[y_test ==1])\n",
    "print \"Unweighted correlation of signal test with mass is\", corr\n",
    "\n",
    "corr = pearsonr(mass_test[(y_pred > threshold) ], y_pred[(y_pred > threshold)])\n",
    "print \"Unweighted correlation of all test events passing cut with mass is\", corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(y_pred>0.5).sum()/float(y_pred.shape[0]) # much better than without class_weight training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "def make_trainable(network, flag):\n",
    "    network.trainable = flag\n",
    "    for l in network.layers:\n",
    "        l.trainable = flag\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "inputs_b = Input(shape=(X_train.shape[1],))\n",
    "inputs_s = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "with K.name_scope('Classifier'):\n",
    "    Dx = Dense(32, activation=\"relu\")(inputs)\n",
    "    Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "    Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "    Dx = Dense(1, activation=\"sigmoid\")(Dx)\n",
    "    D = Model(input=[inputs], output=[Dx])\n",
    "\n",
    "#@TODO: Gradient reversal layer, and simul training\n",
    "#@TODO: loss on only the signal, we want that to be flat\n",
    "Rx = Dx\n",
    "with K.name_scope('Advers'):\n",
    "    Rx = Dense(32, activation=\"relu\")(Rx)\n",
    "    Rx = Dense(32, activation=\"relu\")(Rx)\n",
    "    Rx = Dense(32, activation=\"relu\")(Rx)\n",
    "    #for i in range(20):\n",
    "    #    Rx = Dense(32, activation=\"tanh\")(Rx)\n",
    "    #Rx = Dense(1, activation=\"sigmoid\")(Rx) #try regression activations @TODO\n",
    "    Rx = Dense(1, activation=\"relu\")(Rx)\n",
    "    R = Model(input=[inputs], outputs=[Rx])\n",
    "#@TODO: loss only on background events, tanh activation, batch norm, drop out, see Andreas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "lam = 3.0 #10.0 # pivotal trade-off\n",
    "\n",
    "def make_loss_D(c):\n",
    "    def loss_D(y_true, y_pred):\n",
    "        #return c * K.binary_crossentropy(y_true, y_pred)\n",
    "        return c * K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) #!!! new keras from 2.0.7\n",
    "    return loss_D\n",
    "\n",
    "def make_loss_R(c):\n",
    "    def loss_R(z_true, z_pred):\n",
    "        return c * mean_squared_error(z_true, z_pred) ##!!! new keras from 2.0.7\n",
    "    return loss_R\n",
    "\n",
    "#opt_D = SGD()\n",
    "opt_D = \"adam\"\n",
    "D.compile(loss=[make_loss_D(c=1.0)], optimizer=opt_D)\n",
    "#D.compile(loss=\"binary_crossentropy\", optimizer=opt_D)\n",
    "\n",
    "# Train D such that R loss (its c=-lam) is also minimised, make it invariant to R\n",
    "#can we train simultaneous? grad reversal layer???\n",
    "#opt_DRf = SGD(momentum=0.0)\n",
    "opt_DRf = \"adam\"\n",
    "DRf = Model(input=[inputs], output=[D(inputs), R(inputs)])\n",
    "# R only on signal\n",
    "#DRf = Model(input=[inputs_b, inputs_s], output=[D(inputs_b),D(inputs_s), R(inputs_s)])\n",
    "make_trainable(R, False)\n",
    "make_trainable(D, True)\n",
    "# R only on signal\n",
    "#DRf.compile(loss=[make_loss_D(c=1.0),make_loss_D(c=1.0), make_loss_R(c=-lam)], optimizer=opt_DRf)\n",
    "DRf.compile(loss=[make_loss_D(c=1.0), make_loss_R(c=-lam)], optimizer=opt_DRf)\n",
    "#DRf.compile(loss=[\"binary_crossentropy\", \"mean_squared_error\"], loss_weights=[1,-lam], optimizer=opt_DRf)\n",
    "\n",
    "#opt_DfR = SGD(momentum=0.0)\n",
    "##opt_DfR = \"adam\" #!!!\n",
    "opt_DfR = SGD(lr=0.0000001)\n",
    "DfR = Model(input=[inputs], output=[R(inputs)])\n",
    "make_trainable(R, True)\n",
    "make_trainable(D, False)\n",
    "#DfR.compile(loss=[make_loss_R(c=1.0)], optimizer=opt_DfR)\n",
    "DfR.compile(loss=[make_loss_R(c=lam)], optimizer=opt_DfR)\n",
    "#DfR.compile(loss=[\"mean_squared_error\"], loss_weights=[lam],optimizer=opt_DfR)\n",
    "#DfR.compile(loss=\"mean_squared_error\",optimizer=opt_DfR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, TerminateOnNaN\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(\"D\"), write_images=True\n",
    "                          #, histogram_freq=1,write_grads=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2234 samples, validate on 2234 samples\n",
      "Epoch 1/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.5879 - val_loss: 0.4627\n",
      "Epoch 2/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.4077 - val_loss: 0.3840\n",
      "Epoch 3/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3843 - val_loss: 0.3712\n",
      "Epoch 4/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3746 - val_loss: 0.3607\n",
      "Epoch 5/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3650 - val_loss: 0.3565\n",
      "Epoch 6/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3564 - val_loss: 0.3458\n",
      "Epoch 7/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3497 - val_loss: 0.3409\n",
      "Epoch 8/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3442 - val_loss: 0.3393\n",
      "Epoch 9/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3431 - val_loss: 0.3299\n",
      "Epoch 10/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3369 - val_loss: 0.3256\n",
      "Epoch 11/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3328 - val_loss: 0.3217\n",
      "Epoch 12/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3271 - val_loss: 0.3182\n",
      "Epoch 13/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3258 - val_loss: 0.3153\n",
      "Epoch 14/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3220 - val_loss: 0.3091\n",
      "Epoch 15/15\n",
      "2234/2234 [==============================] - 0s - loss: 0.3192 - val_loss: 0.3074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120a12910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrain D\n",
    "make_trainable(R, False)\n",
    "make_trainable(D, True)\n",
    "##D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=15, callbacks=[tensorboard]) # 15 epochs\n",
    "D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=15, callbacks=[tensorboard],\n",
    "     validation_data=(X_train, y_train, weights_train)) # 15 epochs\n",
    "#D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=10) # 10 epochs\n",
    "\n",
    "#D.fit(X_train, y_train, nb_epoch=10, class_weight=get_class_weights(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2234, 221190)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2234 samples, validate on 2234 samples\n",
      "Epoch 1/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1143\n",
      "Epoch 2/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1143\n",
      "Epoch 3/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1117 - val_loss: 0.1143\n",
      "Epoch 4/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1143\n",
      "Epoch 5/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1143\n",
      "Epoch 6/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1136 - val_loss: 0.1143\n",
      "Epoch 7/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1129 - val_loss: 0.1143\n",
      "Epoch 8/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1142\n",
      "Epoch 9/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1142\n",
      "Epoch 10/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1142\n",
      "Epoch 11/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1122 - val_loss: 0.1142\n",
      "Epoch 12/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1117 - val_loss: 0.1142\n",
      "Epoch 13/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1114 - val_loss: 0.1142\n",
      "Epoch 14/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1103 - val_loss: 0.1142\n",
      "Epoch 15/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1138 - val_loss: 0.1142\n",
      "Epoch 16/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1141\n",
      "Epoch 17/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1141ss: 0.107\n",
      "Epoch 18/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1101 - val_loss: 0.1141\n",
      "Epoch 19/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1141\n",
      "Epoch 20/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1141\n",
      "Epoch 21/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.106 - 0s - loss: 0.1064 - val_loss: 0.1141\n",
      "Epoch 22/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1128 - val_loss: 0.1141\n",
      "Epoch 23/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1141\n",
      "Epoch 24/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1115 - val_loss: 0.1140\n",
      "Epoch 25/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1140\n",
      "Epoch 26/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1103 - val_loss: 0.1140\n",
      "Epoch 27/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1140\n",
      "Epoch 28/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1140\n",
      "Epoch 29/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1106 - val_loss: 0.1140\n",
      "Epoch 30/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1115 - val_loss: 0.1140\n",
      "Epoch 31/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1140\n",
      "Epoch 32/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1129 - val_loss: 0.1140ss: 0.110\n",
      "Epoch 33/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1137 - val_loss: 0.1139\n",
      "Epoch 34/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1132 - val_loss: 0.1139\n",
      "Epoch 35/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1122 - val_loss: 0.1139\n",
      "Epoch 36/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1139\n",
      "Epoch 37/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1135 - val_loss: 0.1139\n",
      "Epoch 38/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1139\n",
      "Epoch 39/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1139\n",
      "Epoch 40/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1139\n",
      "Epoch 41/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1111 - val_loss: 0.1138\n",
      "Epoch 42/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1138\n",
      "Epoch 43/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1138\n",
      "Epoch 44/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1126 - val_loss: 0.1138\n",
      "Epoch 45/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1139 - val_loss: 0.1138\n",
      "Epoch 46/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1138\n",
      "Epoch 47/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1142 - val_loss: 0.1138\n",
      "Epoch 48/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1108 - val_loss: 0.1138\n",
      "Epoch 49/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1142 - val_loss: 0.1138\n",
      "Epoch 50/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1137\n",
      "Epoch 51/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1137\n",
      "Epoch 52/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1111 - val_loss: 0.1137\n",
      "Epoch 53/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1137\n",
      "Epoch 54/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1099 - val_loss: 0.1137\n",
      "Epoch 55/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1137\n",
      "Epoch 56/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1137\n",
      "Epoch 57/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1137\n",
      "Epoch 58/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1118 - val_loss: 0.1136\n",
      "Epoch 59/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1136\n",
      "Epoch 60/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1103 - val_loss: 0.1136\n",
      "Epoch 61/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1099 - val_loss: 0.1136\n",
      "Epoch 62/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1136\n",
      "Epoch 63/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1139 - val_loss: 0.1136\n",
      "Epoch 64/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1138 - val_loss: 0.1136\n",
      "Epoch 65/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1136\n",
      "Epoch 66/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1104 - val_loss: 0.1136\n",
      "Epoch 67/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1121 - val_loss: 0.1135\n",
      "Epoch 68/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1132 - val_loss: 0.1135\n",
      "Epoch 69/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1135\n",
      "Epoch 70/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1123 - val_loss: 0.1135\n",
      "Epoch 71/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1117 - val_loss: 0.1135\n",
      "Epoch 72/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1110 - val_loss: 0.1135\n",
      "Epoch 73/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1135\n",
      "Epoch 74/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1135\n",
      "Epoch 75/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1127 - val_loss: 0.1134\n",
      "Epoch 76/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1108 - val_loss: 0.1134\n",
      "Epoch 77/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1134\n",
      "Epoch 78/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1106 - val_loss: 0.1134\n",
      "Epoch 79/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1134\n",
      "Epoch 80/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1134\n",
      "Epoch 81/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1134\n",
      "Epoch 82/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1134\n",
      "Epoch 83/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1080 - val_loss: 0.1134\n",
      "Epoch 84/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1108 - val_loss: 0.1133\n",
      "Epoch 85/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1133\n",
      "Epoch 86/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1133\n",
      "Epoch 87/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1119 - val_loss: 0.1133\n",
      "Epoch 88/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1110 - val_loss: 0.1133\n",
      "Epoch 89/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1133\n",
      "Epoch 90/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1133\n",
      "Epoch 91/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1133\n",
      "Epoch 92/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1133\n",
      "Epoch 93/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1126 - val_loss: 0.1132\n",
      "Epoch 94/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1122 - val_loss: 0.1132\n",
      "Epoch 95/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1141 - val_loss: 0.1132\n",
      "Epoch 96/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1132\n",
      "Epoch 97/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1133 - val_loss: 0.1132\n",
      "Epoch 98/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1132\n",
      "Epoch 99/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1132\n",
      "Epoch 100/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1132\n",
      "Epoch 101/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1131\n",
      "Epoch 102/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1131\n",
      "Epoch 103/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1133 - val_loss: 0.1131\n",
      "Epoch 104/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1131\n",
      "Epoch 105/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1150 - val_loss: 0.1131\n",
      "Epoch 106/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1131\n",
      "Epoch 107/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1131\n",
      "Epoch 108/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1131\n",
      "Epoch 109/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1131\n",
      "Epoch 110/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1130\n",
      "Epoch 111/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1108 - val_loss: 0.1130\n",
      "Epoch 112/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1130\n",
      "Epoch 113/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1116 - val_loss: 0.1130\n",
      "Epoch 114/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1130\n",
      "Epoch 115/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1130\n",
      "Epoch 116/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1130\n",
      "Epoch 117/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1130\n",
      "Epoch 118/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1130\n",
      "Epoch 119/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1129\n",
      "Epoch 120/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1129\n",
      "Epoch 121/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1129\n",
      "Epoch 122/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1104 - val_loss: 0.1129\n",
      "Epoch 123/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1129\n",
      "Epoch 124/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1129\n",
      "Epoch 125/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1108 - val_loss: 0.1129\n",
      "Epoch 126/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1118 - val_loss: 0.1129\n",
      "Epoch 127/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1129\n",
      "Epoch 128/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1129 - val_loss: 0.1128\n",
      "Epoch 129/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1128\n",
      "Epoch 130/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1128\n",
      "Epoch 131/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1128\n",
      "Epoch 132/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1116 - val_loss: 0.1128\n",
      "Epoch 133/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1109 - val_loss: 0.1128\n",
      "Epoch 134/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1128\n",
      "Epoch 135/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1110 - val_loss: 0.1128\n",
      "Epoch 136/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1128\n",
      "Epoch 137/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1144 - val_loss: 0.1127\n",
      "Epoch 138/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.111 - 0s - loss: 0.1082 - val_loss: 0.1127\n",
      "Epoch 139/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1127\n",
      "Epoch 140/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1127\n",
      "Epoch 141/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1131 - val_loss: 0.1127\n",
      "Epoch 142/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1127\n",
      "Epoch 143/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1127\n",
      "Epoch 144/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1127\n",
      "Epoch 145/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1126\n",
      "Epoch 146/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1126\n",
      "Epoch 147/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1126\n",
      "Epoch 148/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1126\n",
      "Epoch 149/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1126\n",
      "Epoch 150/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1117 - val_loss: 0.1126\n",
      "Epoch 151/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1126\n",
      "Epoch 152/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1126\n",
      "Epoch 153/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1126\n",
      "Epoch 154/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1123 - val_loss: 0.1125\n",
      "Epoch 155/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1125\n",
      "Epoch 156/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1111 - val_loss: 0.1125\n",
      "Epoch 157/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1125\n",
      "Epoch 158/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1125\n",
      "Epoch 159/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1125\n",
      "Epoch 160/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1125\n",
      "Epoch 161/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1125\n",
      "Epoch 162/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1125\n",
      "Epoch 163/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1124\n",
      "Epoch 164/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1105 - val_loss: 0.1124\n",
      "Epoch 165/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1124\n",
      "Epoch 166/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1124\n",
      "Epoch 167/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1124\n",
      "Epoch 168/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1124\n",
      "Epoch 169/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1124\n",
      "Epoch 170/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1124\n",
      "Epoch 171/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1124\n",
      "Epoch 172/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1140 - val_loss: 0.1123\n",
      "Epoch 173/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1123\n",
      "Epoch 174/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1123\n",
      "Epoch 175/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1123\n",
      "Epoch 176/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1123\n",
      "Epoch 177/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1123\n",
      "Epoch 178/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1123\n",
      "Epoch 179/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1123\n",
      "Epoch 180/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1123\n",
      "Epoch 181/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1123\n",
      "Epoch 182/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1122\n",
      "Epoch 183/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1122\n",
      "Epoch 184/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1119 - val_loss: 0.1122\n",
      "Epoch 185/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1122\n",
      "Epoch 186/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1117 - val_loss: 0.1122\n",
      "Epoch 187/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1122\n",
      "Epoch 188/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1122\n",
      "Epoch 189/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1122\n",
      "Epoch 190/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.105 - 0s - loss: 0.1054 - val_loss: 0.1122\n",
      "Epoch 191/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1121\n",
      "Epoch 192/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1121\n",
      "Epoch 193/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1121\n",
      "Epoch 194/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1121\n",
      "Epoch 195/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1121\n",
      "Epoch 196/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1121\n",
      "Epoch 197/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1121\n",
      "Epoch 198/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1121\n",
      "Epoch 199/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1121\n",
      "Epoch 200/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1120\n",
      "Epoch 201/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1120\n",
      "Epoch 202/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1120\n",
      "Epoch 203/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1120\n",
      "Epoch 204/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1120\n",
      "Epoch 205/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1120\n",
      "Epoch 206/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1120\n",
      "Epoch 207/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1120\n",
      "Epoch 208/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1120\n",
      "Epoch 209/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1119\n",
      "Epoch 210/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1129 - val_loss: 0.1119\n",
      "Epoch 211/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1119ss: 0.10\n",
      "Epoch 212/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1119\n",
      "Epoch 213/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1119\n",
      "Epoch 214/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1119\n",
      "Epoch 215/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1070 - val_loss: 0.1119\n",
      "Epoch 216/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1119\n",
      "Epoch 217/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1119\n",
      "Epoch 218/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1119\n",
      "Epoch 219/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1118\n",
      "Epoch 220/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1118\n",
      "Epoch 221/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1118ss: 0.099 - ETA: 0s - loss: 0.107\n",
      "Epoch 222/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1118\n",
      "Epoch 223/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1118\n",
      "Epoch 224/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1118\n",
      "Epoch 225/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1118\n",
      "Epoch 226/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1118\n",
      "Epoch 227/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1118\n",
      "Epoch 228/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1117\n",
      "Epoch 229/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1117\n",
      "Epoch 230/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1117\n",
      "Epoch 231/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1117\n",
      "Epoch 232/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1073 - val_loss: 0.1117\n",
      "Epoch 233/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1117\n",
      "Epoch 234/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1119 - val_loss: 0.1117\n",
      "Epoch 235/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1117\n",
      "Epoch 236/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1117\n",
      "Epoch 237/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1133 - val_loss: 0.1117\n",
      "Epoch 238/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1110 - val_loss: 0.1116\n",
      "Epoch 239/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1116\n",
      "Epoch 240/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1116\n",
      "Epoch 241/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1116\n",
      "Epoch 242/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1116\n",
      "Epoch 243/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1116\n",
      "Epoch 244/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1116\n",
      "Epoch 245/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1116\n",
      "Epoch 246/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1107 - val_loss: 0.1116\n",
      "Epoch 247/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1114 - val_loss: 0.1115\n",
      "Epoch 248/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1114 - val_loss: 0.1115\n",
      "Epoch 249/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1115\n",
      "Epoch 250/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1115\n",
      "Epoch 251/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1115\n",
      "Epoch 252/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1115\n",
      "Epoch 253/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1115\n",
      "Epoch 254/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1115\n",
      "Epoch 255/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1115\n",
      "Epoch 256/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1115\n",
      "Epoch 257/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1114\n",
      "Epoch 258/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1114\n",
      "Epoch 259/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1104 - val_loss: 0.1114\n",
      "Epoch 260/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1114\n",
      "Epoch 261/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1114\n",
      "Epoch 262/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1073 - val_loss: 0.1114\n",
      "Epoch 263/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1114\n",
      "Epoch 264/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1114\n",
      "Epoch 265/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1114\n",
      "Epoch 266/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1114\n",
      "Epoch 267/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1113\n",
      "Epoch 268/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1113\n",
      "Epoch 269/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1113\n",
      "Epoch 270/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1113\n",
      "Epoch 271/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1113\n",
      "Epoch 272/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1113\n",
      "Epoch 273/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1113\n",
      "Epoch 274/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1113\n",
      "Epoch 275/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1113\n",
      "Epoch 276/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1101 - val_loss: 0.1112\n",
      "Epoch 277/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1112\n",
      "Epoch 278/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1112\n",
      "Epoch 279/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1112\n",
      "Epoch 280/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1112\n",
      "Epoch 281/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1112\n",
      "Epoch 282/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1112\n",
      "Epoch 283/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1112\n",
      "Epoch 284/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1112\n",
      "Epoch 285/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1112\n",
      "Epoch 286/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1111\n",
      "Epoch 287/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1111\n",
      "Epoch 288/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1111\n",
      "Epoch 289/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1111\n",
      "Epoch 290/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1111\n",
      "Epoch 291/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1103 - val_loss: 0.1111ss: 0.109\n",
      "Epoch 292/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1111\n",
      "Epoch 293/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1121 - val_loss: 0.1111\n",
      "Epoch 294/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1111\n",
      "Epoch 295/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1111\n",
      "Epoch 296/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1110\n",
      "Epoch 297/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1073 - val_loss: 0.1110\n",
      "Epoch 298/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1110\n",
      "Epoch 299/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1110\n",
      "Epoch 300/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1110\n",
      "Epoch 301/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1085 - val_loss: 0.1110\n",
      "Epoch 302/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1110\n",
      "Epoch 303/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1111 - val_loss: 0.1110\n",
      "Epoch 304/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1110\n",
      "Epoch 305/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1073 - val_loss: 0.1110\n",
      "Epoch 306/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1109\n",
      "Epoch 307/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1070 - val_loss: 0.1109\n",
      "Epoch 308/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1109\n",
      "Epoch 309/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1109\n",
      "Epoch 310/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1109\n",
      "Epoch 311/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1109\n",
      "Epoch 312/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1109\n",
      "Epoch 313/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1109\n",
      "Epoch 314/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1109\n",
      "Epoch 315/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1109\n",
      "Epoch 316/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1109\n",
      "Epoch 317/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1108\n",
      "Epoch 318/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1108\n",
      "Epoch 319/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1108\n",
      "Epoch 320/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1108\n",
      "Epoch 321/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1108\n",
      "Epoch 322/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1108\n",
      "Epoch 323/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1108\n",
      "Epoch 324/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1108\n",
      "Epoch 325/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1108\n",
      "Epoch 326/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1108\n",
      "Epoch 327/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1080 - val_loss: 0.1107\n",
      "Epoch 328/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1107\n",
      "Epoch 329/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1107\n",
      "Epoch 330/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1107\n",
      "Epoch 331/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1107\n",
      "Epoch 332/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1091 - val_loss: 0.1107\n",
      "Epoch 333/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1107\n",
      "Epoch 334/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1107\n",
      "Epoch 335/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1110 - val_loss: 0.1107\n",
      "Epoch 336/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1107\n",
      "Epoch 337/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1106\n",
      "Epoch 338/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1106\n",
      "Epoch 339/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1106\n",
      "Epoch 340/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1106\n",
      "Epoch 341/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1106\n",
      "Epoch 342/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1106\n",
      "Epoch 343/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1106\n",
      "Epoch 344/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1106\n",
      "Epoch 345/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1106\n",
      "Epoch 346/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1106\n",
      "Epoch 347/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1105\n",
      "Epoch 348/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1105\n",
      "Epoch 349/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1105\n",
      "Epoch 350/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1105\n",
      "Epoch 351/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1105\n",
      "Epoch 352/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1065 - val_loss: 0.1105\n",
      "Epoch 353/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1070 - val_loss: 0.1105\n",
      "Epoch 354/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1105\n",
      "Epoch 355/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1105\n",
      "Epoch 356/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1105\n",
      "Epoch 357/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1104\n",
      "Epoch 358/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1104\n",
      "Epoch 359/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1104\n",
      "Epoch 360/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1104\n",
      "Epoch 361/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1104\n",
      "Epoch 362/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1104\n",
      "Epoch 363/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1104\n",
      "Epoch 364/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1104\n",
      "Epoch 365/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1070 - val_loss: 0.1104\n",
      "Epoch 366/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1104\n",
      "Epoch 367/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1104ss: 0.100\n",
      "Epoch 368/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1103\n",
      "Epoch 369/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1103\n",
      "Epoch 370/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1103\n",
      "Epoch 371/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1103\n",
      "Epoch 372/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1103\n",
      "Epoch 373/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1103\n",
      "Epoch 374/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1103\n",
      "Epoch 375/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1103ss: 0.099\n",
      "Epoch 376/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1103ss: 0.09\n",
      "Epoch 377/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1103\n",
      "Epoch 378/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1102ss: 0.110\n",
      "Epoch 379/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1102\n",
      "Epoch 380/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1102\n",
      "Epoch 381/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1102\n",
      "Epoch 382/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1102\n",
      "Epoch 383/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1102\n",
      "Epoch 384/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1094 - val_loss: 0.1102\n",
      "Epoch 385/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1102\n",
      "Epoch 386/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1102\n",
      "Epoch 387/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1109 - val_loss: 0.1102\n",
      "Epoch 388/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1101\n",
      "Epoch 389/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1101\n",
      "Epoch 390/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1101\n",
      "Epoch 391/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1101\n",
      "Epoch 392/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1101\n",
      "Epoch 393/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1101\n",
      "Epoch 394/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1101\n",
      "Epoch 395/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1080 - val_loss: 0.1101\n",
      "Epoch 396/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1070 - val_loss: 0.1101\n",
      "Epoch 397/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1101\n",
      "Epoch 398/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1100\n",
      "Epoch 399/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1100\n",
      "Epoch 400/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1100\n",
      "Epoch 401/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1100\n",
      "Epoch 402/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1100\n",
      "Epoch 403/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1100\n",
      "Epoch 404/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1046 - val_loss: 0.1100\n",
      "Epoch 405/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1100\n",
      "Epoch 406/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1100\n",
      "Epoch 407/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1100\n",
      "Epoch 408/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1099\n",
      "Epoch 409/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1099\n",
      "Epoch 410/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1099\n",
      "Epoch 411/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1080 - val_loss: 0.1099\n",
      "Epoch 412/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1097 - val_loss: 0.1099\n",
      "Epoch 413/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1099\n",
      "Epoch 414/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1099\n",
      "Epoch 415/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1099\n",
      "Epoch 416/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1099\n",
      "Epoch 417/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1099\n",
      "Epoch 418/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1099\n",
      "Epoch 419/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1098\n",
      "Epoch 420/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1098\n",
      "Epoch 421/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1098\n",
      "Epoch 422/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1098\n",
      "Epoch 423/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1089 - val_loss: 0.1098\n",
      "Epoch 424/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1112 - val_loss: 0.1098\n",
      "Epoch 425/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1098\n",
      "Epoch 426/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1098\n",
      "Epoch 427/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1098\n",
      "Epoch 428/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1100 - val_loss: 0.1098\n",
      "Epoch 429/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1101 - val_loss: 0.1097\n",
      "Epoch 430/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1097\n",
      "Epoch 431/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1097\n",
      "Epoch 432/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1097\n",
      "Epoch 433/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1097\n",
      "Epoch 434/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1079 - val_loss: 0.1097\n",
      "Epoch 435/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1097\n",
      "Epoch 436/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1121 - val_loss: 0.1097\n",
      "Epoch 437/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1097\n",
      "Epoch 438/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1097\n",
      "Epoch 439/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1095 - val_loss: 0.1096\n",
      "Epoch 440/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1096\n",
      "Epoch 441/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1096\n",
      "Epoch 442/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1096\n",
      "Epoch 443/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1096\n",
      "Epoch 444/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1084 - val_loss: 0.1096\n",
      "Epoch 445/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1096\n",
      "Epoch 446/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1033 - val_loss: 0.1096\n",
      "Epoch 447/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1033 - val_loss: 0.1096\n",
      "Epoch 448/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1096\n",
      "Epoch 449/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1096\n",
      "Epoch 450/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1095\n",
      "Epoch 451/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1095\n",
      "Epoch 452/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1095\n",
      "Epoch 453/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1095\n",
      "Epoch 454/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1095\n",
      "Epoch 455/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1095\n",
      "Epoch 456/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1095\n",
      "Epoch 457/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1095\n",
      "Epoch 458/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1109 - val_loss: 0.1095\n",
      "Epoch 459/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1095\n",
      "Epoch 460/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1094\n",
      "Epoch 461/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1094\n",
      "Epoch 462/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1094\n",
      "Epoch 463/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1094\n",
      "Epoch 464/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1094\n",
      "Epoch 465/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1094\n",
      "Epoch 466/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1094\n",
      "Epoch 467/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1094\n",
      "Epoch 468/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1094\n",
      "Epoch 469/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1094\n",
      "Epoch 470/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1093\n",
      "Epoch 471/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1093\n",
      "Epoch 472/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1093\n",
      "Epoch 473/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1093\n",
      "Epoch 474/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1093\n",
      "Epoch 475/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1093\n",
      "Epoch 476/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1093\n",
      "Epoch 477/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1093\n",
      "Epoch 478/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1093\n",
      "Epoch 479/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1093\n",
      "Epoch 480/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1093\n",
      "Epoch 481/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1092\n",
      "Epoch 482/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1092\n",
      "Epoch 483/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1092\n",
      "Epoch 484/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1092\n",
      "Epoch 485/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1092\n",
      "Epoch 486/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1092\n",
      "Epoch 487/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1092\n",
      "Epoch 488/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1092\n",
      "Epoch 489/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1092\n",
      "Epoch 490/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1092\n",
      "Epoch 491/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1091\n",
      "Epoch 492/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1091\n",
      "Epoch 493/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1091\n",
      "Epoch 494/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1091\n",
      "Epoch 495/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1091\n",
      "Epoch 496/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1091\n",
      "Epoch 497/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1091\n",
      "Epoch 498/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1091\n",
      "Epoch 499/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1091\n",
      "Epoch 500/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1091\n",
      "Epoch 501/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1091\n",
      "Epoch 502/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1090\n",
      "Epoch 503/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1090\n",
      "Epoch 504/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1102 - val_loss: 0.1090\n",
      "Epoch 505/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1090\n",
      "Epoch 506/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1090\n",
      "Epoch 507/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1090\n",
      "Epoch 508/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1110 - val_loss: 0.1090\n",
      "Epoch 509/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1090\n",
      "Epoch 510/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1090\n",
      "Epoch 511/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1090\n",
      "Epoch 512/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1089\n",
      "Epoch 513/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1089\n",
      "Epoch 514/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1089\n",
      "Epoch 515/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1089\n",
      "Epoch 516/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1089\n",
      "Epoch 517/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1089\n",
      "Epoch 518/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1089\n",
      "Epoch 519/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1089\n",
      "Epoch 520/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1089\n",
      "Epoch 521/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1089\n",
      "Epoch 522/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1088\n",
      "Epoch 523/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.103 - 0s - loss: 0.1057 - val_loss: 0.1088\n",
      "Epoch 524/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1096 - val_loss: 0.1088\n",
      "Epoch 525/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1088\n",
      "Epoch 526/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1088\n",
      "Epoch 527/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1088\n",
      "Epoch 528/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1088\n",
      "Epoch 529/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1088\n",
      "Epoch 530/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1088\n",
      "Epoch 531/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1088\n",
      "Epoch 532/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1088\n",
      "Epoch 533/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1087\n",
      "Epoch 534/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1087\n",
      "Epoch 535/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1087\n",
      "Epoch 536/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1087\n",
      "Epoch 537/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1087\n",
      "Epoch 538/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1087\n",
      "Epoch 539/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1137 - val_loss: 0.1087\n",
      "Epoch 540/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1087\n",
      "Epoch 541/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1087\n",
      "Epoch 542/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1070 - val_loss: 0.1087\n",
      "Epoch 543/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1086\n",
      "Epoch 544/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1114 - val_loss: 0.1086\n",
      "Epoch 545/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1086\n",
      "Epoch 546/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1086\n",
      "Epoch 547/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1073 - val_loss: 0.1086\n",
      "Epoch 548/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1086\n",
      "Epoch 549/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1086\n",
      "Epoch 550/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1086\n",
      "Epoch 551/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1086\n",
      "Epoch 552/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1086\n",
      "Epoch 553/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1086\n",
      "Epoch 554/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1033 - val_loss: 0.1085\n",
      "Epoch 555/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1085\n",
      "Epoch 556/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1085\n",
      "Epoch 557/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1085\n",
      "Epoch 558/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1085\n",
      "Epoch 559/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1085\n",
      "Epoch 560/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1085\n",
      "Epoch 561/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1085\n",
      "Epoch 562/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1085\n",
      "Epoch 563/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1085\n",
      "Epoch 564/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1084\n",
      "Epoch 565/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1084\n",
      "Epoch 566/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1084\n",
      "Epoch 567/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1084\n",
      "Epoch 568/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1084\n",
      "Epoch 569/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1084\n",
      "Epoch 570/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1084\n",
      "Epoch 571/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1084\n",
      "Epoch 572/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1084\n",
      "Epoch 573/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1084\n",
      "Epoch 574/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1084\n",
      "Epoch 575/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1083\n",
      "Epoch 576/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1083\n",
      "Epoch 577/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1083\n",
      "Epoch 578/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1083\n",
      "Epoch 579/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1083\n",
      "Epoch 580/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1083\n",
      "Epoch 581/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1083\n",
      "Epoch 582/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1083ss: 0.09\n",
      "Epoch 583/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1083\n",
      "Epoch 584/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1083\n",
      "Epoch 585/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1082\n",
      "Epoch 586/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1082\n",
      "Epoch 587/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1082\n",
      "Epoch 588/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1081 - val_loss: 0.1082\n",
      "Epoch 589/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1082\n",
      "Epoch 590/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1082\n",
      "Epoch 591/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1082\n",
      "Epoch 592/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1078 - val_loss: 0.1082\n",
      "Epoch 593/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1035 - val_loss: 0.1082\n",
      "Epoch 594/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1082\n",
      "Epoch 595/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1082\n",
      "Epoch 596/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1081\n",
      "Epoch 597/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1081\n",
      "Epoch 598/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1081\n",
      "Epoch 599/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1081\n",
      "Epoch 600/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1046 - val_loss: 0.1081\n",
      "Epoch 601/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1081\n",
      "Epoch 602/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1081\n",
      "Epoch 603/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1081\n",
      "Epoch 604/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1081\n",
      "Epoch 605/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1081\n",
      "Epoch 606/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1081\n",
      "Epoch 607/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1080\n",
      "Epoch 608/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1080\n",
      "Epoch 609/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1080\n",
      "Epoch 610/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1080\n",
      "Epoch 611/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1080\n",
      "Epoch 612/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1080\n",
      "Epoch 613/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1080\n",
      "Epoch 614/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1030 - val_loss: 0.1080\n",
      "Epoch 615/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1080\n",
      "Epoch 616/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1080\n",
      "Epoch 617/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1079\n",
      "Epoch 618/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1079\n",
      "Epoch 619/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1079\n",
      "Epoch 620/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1079\n",
      "Epoch 621/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1079\n",
      "Epoch 622/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1079\n",
      "Epoch 623/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1079\n",
      "Epoch 624/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1079\n",
      "Epoch 625/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1079\n",
      "Epoch 626/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1079\n",
      "Epoch 627/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1079\n",
      "Epoch 628/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1078\n",
      "Epoch 629/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1062 - val_loss: 0.1078\n",
      "Epoch 630/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1078\n",
      "Epoch 631/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1078\n",
      "Epoch 632/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1078\n",
      "Epoch 633/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1078\n",
      "Epoch 634/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1078\n",
      "Epoch 635/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1078\n",
      "Epoch 636/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1078\n",
      "Epoch 637/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1078\n",
      "Epoch 638/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1058 - val_loss: 0.1077\n",
      "Epoch 639/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1077\n",
      "Epoch 640/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.102 - 0s - loss: 0.1080 - val_loss: 0.1077\n",
      "Epoch 641/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1077\n",
      "Epoch 642/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1077\n",
      "Epoch 643/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1077\n",
      "Epoch 644/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1046 - val_loss: 0.1077\n",
      "Epoch 645/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1077\n",
      "Epoch 646/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1077\n",
      "Epoch 647/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1077\n",
      "Epoch 648/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1077\n",
      "Epoch 649/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1080 - val_loss: 0.1076\n",
      "Epoch 650/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1080 - val_loss: 0.1076\n",
      "Epoch 651/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1076\n",
      "Epoch 652/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1076\n",
      "Epoch 653/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.101 - 0s - loss: 0.1040 - val_loss: 0.1076\n",
      "Epoch 654/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1030 - val_loss: 0.1076\n",
      "Epoch 655/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1068 - val_loss: 0.1076\n",
      "Epoch 656/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1076\n",
      "Epoch 657/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1035 - val_loss: 0.1076\n",
      "Epoch 658/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1054 - val_loss: 0.1076\n",
      "Epoch 659/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1076\n",
      "Epoch 660/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1033 - val_loss: 0.1075\n",
      "Epoch 661/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1075\n",
      "Epoch 662/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1031 - val_loss: 0.1075\n",
      "Epoch 663/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1075\n",
      "Epoch 664/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1075\n",
      "Epoch 665/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1075\n",
      "Epoch 666/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1075\n",
      "Epoch 667/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1075\n",
      "Epoch 668/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1075\n",
      "Epoch 669/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1075\n",
      "Epoch 670/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1074\n",
      "Epoch 671/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1074\n",
      "Epoch 672/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1074\n",
      "Epoch 673/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1074\n",
      "Epoch 674/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1074\n",
      "Epoch 675/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1074\n",
      "Epoch 676/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1074\n",
      "Epoch 677/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1074\n",
      "Epoch 678/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1074\n",
      "Epoch 679/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1074\n",
      "Epoch 680/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1074\n",
      "Epoch 681/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1073\n",
      "Epoch 682/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1073\n",
      "Epoch 683/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1073\n",
      "Epoch 684/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1073\n",
      "Epoch 685/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1014 - val_loss: 0.1073\n",
      "Epoch 686/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1073\n",
      "Epoch 687/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1073\n",
      "Epoch 688/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1073\n",
      "Epoch 689/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1073\n",
      "Epoch 690/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1073ss: 0.11\n",
      "Epoch 691/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1073\n",
      "Epoch 692/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1066 - val_loss: 0.1072\n",
      "Epoch 693/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.108 - 0s - loss: 0.1056 - val_loss: 0.1072\n",
      "Epoch 694/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1072\n",
      "Epoch 695/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1072\n",
      "Epoch 696/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1072\n",
      "Epoch 697/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1072\n",
      "Epoch 698/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1060 - val_loss: 0.1072\n",
      "Epoch 699/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1035 - val_loss: 0.1072\n",
      "Epoch 700/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1072\n",
      "Epoch 701/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1072\n",
      "Epoch 702/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1072\n",
      "Epoch 703/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1071\n",
      "Epoch 704/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1071\n",
      "Epoch 705/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1071\n",
      "Epoch 706/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1071\n",
      "Epoch 707/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1071\n",
      "Epoch 708/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1071\n",
      "Epoch 709/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1049 - val_loss: 0.1071\n",
      "Epoch 710/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0995 - val_loss: 0.1071\n",
      "Epoch 711/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1071\n",
      "Epoch 712/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1071\n",
      "Epoch 713/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1070\n",
      "Epoch 714/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1070\n",
      "Epoch 715/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1070\n",
      "Epoch 716/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1070\n",
      "Epoch 717/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1070\n",
      "Epoch 718/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1070\n",
      "Epoch 719/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1070\n",
      "Epoch 720/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1070\n",
      "Epoch 721/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1070\n",
      "Epoch 722/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1070\n",
      "Epoch 723/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1046 - val_loss: 0.1070\n",
      "Epoch 724/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1069\n",
      "Epoch 725/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1064 - val_loss: 0.1069\n",
      "Epoch 726/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1069\n",
      "Epoch 727/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1069\n",
      "Epoch 728/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1069\n",
      "Epoch 729/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1007 - val_loss: 0.1069\n",
      "Epoch 730/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1069\n",
      "Epoch 731/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1069\n",
      "Epoch 732/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1069\n",
      "Epoch 733/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0990 - val_loss: 0.1069\n",
      "Epoch 734/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1069\n",
      "Epoch 735/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.107 - 0s - loss: 0.1035 - val_loss: 0.1068\n",
      "Epoch 736/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1007 - val_loss: 0.1068\n",
      "Epoch 737/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1068\n",
      "Epoch 738/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1076 - val_loss: 0.1068\n",
      "Epoch 739/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1068\n",
      "Epoch 740/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1068\n",
      "Epoch 741/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1006 - val_loss: 0.1068\n",
      "Epoch 742/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1068\n",
      "Epoch 743/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1068\n",
      "Epoch 744/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1067 - val_loss: 0.1068\n",
      "Epoch 745/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1068\n",
      "Epoch 746/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1067\n",
      "Epoch 747/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1043 - val_loss: 0.1067\n",
      "Epoch 748/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1067\n",
      "Epoch 749/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1067\n",
      "Epoch 750/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1067\n",
      "Epoch 751/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1067\n",
      "Epoch 752/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1067\n",
      "Epoch 753/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1030 - val_loss: 0.1067\n",
      "Epoch 754/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1046 - val_loss: 0.1067\n",
      "Epoch 755/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1067\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.0995 - val_loss: 0.1067\n",
      "Epoch 757/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1066\n",
      "Epoch 758/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1066\n",
      "Epoch 759/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1066\n",
      "Epoch 760/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1035 - val_loss: 0.1066\n",
      "Epoch 761/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1066\n",
      "Epoch 762/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1035 - val_loss: 0.1066\n",
      "Epoch 763/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1066\n",
      "Epoch 764/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1066\n",
      "Epoch 765/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1011 - val_loss: 0.1066\n",
      "Epoch 766/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1113 - val_loss: 0.1066\n",
      "Epoch 767/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1066\n",
      "Epoch 768/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1065\n",
      "Epoch 769/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1065\n",
      "Epoch 770/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1065\n",
      "Epoch 771/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1065\n",
      "Epoch 772/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1065\n",
      "Epoch 773/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1065\n",
      "Epoch 774/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1065\n",
      "Epoch 775/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1065\n",
      "Epoch 776/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1065\n",
      "Epoch 777/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1007 - val_loss: 0.1065\n",
      "Epoch 778/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1064\n",
      "Epoch 779/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1006 - val_loss: 0.1064\n",
      "Epoch 780/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1019 - val_loss: 0.1064\n",
      "Epoch 781/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.108 - 0s - loss: 0.1021 - val_loss: 0.1064\n",
      "Epoch 782/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1019 - val_loss: 0.1064\n",
      "Epoch 783/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1064\n",
      "Epoch 784/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1092 - val_loss: 0.1064\n",
      "Epoch 785/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1064\n",
      "Epoch 786/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1064\n",
      "Epoch 787/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1075 - val_loss: 0.1064\n",
      "Epoch 788/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1064\n",
      "Epoch 789/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1063\n",
      "Epoch 790/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1063\n",
      "Epoch 791/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1003 - val_loss: 0.1063\n",
      "Epoch 792/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1019 - val_loss: 0.1063\n",
      "Epoch 793/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1063\n",
      "Epoch 794/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1074 - val_loss: 0.1063\n",
      "Epoch 795/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1063\n",
      "Epoch 796/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1063\n",
      "Epoch 797/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1063\n",
      "Epoch 798/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1045 - val_loss: 0.1063\n",
      "Epoch 799/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1063\n",
      "Epoch 800/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1062\n",
      "Epoch 801/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1062\n",
      "Epoch 802/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1062\n",
      "Epoch 803/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1062\n",
      "Epoch 804/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1062\n",
      "Epoch 805/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1062\n",
      "Epoch 806/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1062\n",
      "Epoch 807/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1014 - val_loss: 0.1062\n",
      "Epoch 808/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1062\n",
      "Epoch 809/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1062\n",
      "Epoch 810/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1014 - val_loss: 0.1062\n",
      "Epoch 811/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1061\n",
      "Epoch 812/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1061\n",
      "Epoch 813/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1061\n",
      "Epoch 814/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0998 - val_loss: 0.1061\n",
      "Epoch 815/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1061\n",
      "Epoch 816/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1061\n",
      "Epoch 817/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1008 - val_loss: 0.1061\n",
      "Epoch 818/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1061\n",
      "Epoch 819/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1061\n",
      "Epoch 820/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1061\n",
      "Epoch 821/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1031 - val_loss: 0.1061\n",
      "Epoch 822/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1060\n",
      "Epoch 823/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1060\n",
      "Epoch 824/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1060\n",
      "Epoch 825/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1060\n",
      "Epoch 826/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1060\n",
      "Epoch 827/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1060\n",
      "Epoch 828/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1060\n",
      "Epoch 829/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1060\n",
      "Epoch 830/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1061 - val_loss: 0.1060\n",
      "Epoch 831/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1060\n",
      "Epoch 832/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1011 - val_loss: 0.1060\n",
      "Epoch 833/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1059\n",
      "Epoch 834/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1059\n",
      "Epoch 835/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1059\n",
      "Epoch 836/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1059\n",
      "Epoch 837/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1059\n",
      "Epoch 838/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0995 - val_loss: 0.1059\n",
      "Epoch 839/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1059\n",
      "Epoch 840/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1055 - val_loss: 0.1059\n",
      "Epoch 841/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0989 - val_loss: 0.1059\n",
      "Epoch 842/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1059\n",
      "Epoch 843/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1059\n",
      "Epoch 844/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1058\n",
      "Epoch 845/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1058\n",
      "Epoch 846/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1030 - val_loss: 0.1058\n",
      "Epoch 847/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1058\n",
      "Epoch 848/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1058\n",
      "Epoch 849/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1058\n",
      "Epoch 850/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1058\n",
      "Epoch 851/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1058\n",
      "Epoch 852/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1071 - val_loss: 0.1058\n",
      "Epoch 853/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1058ss: 0.08\n",
      "Epoch 854/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1004 - val_loss: 0.1058\n",
      "Epoch 855/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1057\n",
      "Epoch 856/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1057\n",
      "Epoch 857/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1057\n",
      "Epoch 858/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1057\n",
      "Epoch 859/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1057\n",
      "Epoch 860/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1057\n",
      "Epoch 861/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0998 - val_loss: 0.1057\n",
      "Epoch 862/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1004 - val_loss: 0.1057\n",
      "Epoch 863/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1057\n",
      "Epoch 864/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1057\n",
      "Epoch 865/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1057\n",
      "Epoch 866/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1056\n",
      "Epoch 867/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1056\n",
      "Epoch 868/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1056\n",
      "Epoch 869/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1050 - val_loss: 0.1056\n",
      "Epoch 870/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1056\n",
      "Epoch 871/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1052 - val_loss: 0.1056\n",
      "Epoch 872/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1056\n",
      "Epoch 873/1000\n",
      "2234/2234 [==============================] - ETA: 0s - loss: 0.106 - 0s - loss: 0.1055 - val_loss: 0.1056\n",
      "Epoch 874/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1031 - val_loss: 0.1056\n",
      "Epoch 875/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1031 - val_loss: 0.1056\n",
      "Epoch 876/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1056\n",
      "Epoch 877/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1055\n",
      "Epoch 878/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1055\n",
      "Epoch 879/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1123 - val_loss: 0.1055\n",
      "Epoch 880/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1055\n",
      "Epoch 881/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1055\n",
      "Epoch 882/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1055\n",
      "Epoch 883/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1055\n",
      "Epoch 884/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1055\n",
      "Epoch 885/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1055\n",
      "Epoch 886/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1055\n",
      "Epoch 887/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0994 - val_loss: 0.1055\n",
      "Epoch 888/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1054\n",
      "Epoch 889/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1054\n",
      "Epoch 890/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1054\n",
      "Epoch 891/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1054\n",
      "Epoch 892/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1054\n",
      "Epoch 893/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1054\n",
      "Epoch 894/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1054\n",
      "Epoch 895/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1054\n",
      "Epoch 896/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1054\n",
      "Epoch 897/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1054\n",
      "Epoch 898/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1054\n",
      "Epoch 899/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1053\n",
      "Epoch 900/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1053\n",
      "Epoch 901/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1053\n",
      "Epoch 902/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1053\n",
      "Epoch 903/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1053\n",
      "Epoch 904/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1053\n",
      "Epoch 905/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1053\n",
      "Epoch 906/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1053\n",
      "Epoch 907/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1053\n",
      "Epoch 908/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1053\n",
      "Epoch 909/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1053\n",
      "Epoch 910/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1052\n",
      "Epoch 911/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0999 - val_loss: 0.1052\n",
      "Epoch 912/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0986 - val_loss: 0.1052\n",
      "Epoch 913/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1052\n",
      "Epoch 914/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1052\n",
      "Epoch 915/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1024 - val_loss: 0.1052\n",
      "Epoch 916/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1052\n",
      "Epoch 917/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1052\n",
      "Epoch 918/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1052\n",
      "Epoch 919/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1052\n",
      "Epoch 920/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1052\n",
      "Epoch 921/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1051\n",
      "Epoch 922/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1051\n",
      "Epoch 923/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1051\n",
      "Epoch 924/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234/2234 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1051\n",
      "Epoch 925/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1018 - val_loss: 0.1051\n",
      "Epoch 926/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0988 - val_loss: 0.1051\n",
      "Epoch 927/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1051\n",
      "Epoch 928/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1051\n",
      "Epoch 929/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1051\n",
      "Epoch 930/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1033 - val_loss: 0.1051\n",
      "Epoch 931/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1051\n",
      "Epoch 932/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1007 - val_loss: 0.1050\n",
      "Epoch 933/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1050\n",
      "Epoch 934/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1008 - val_loss: 0.1050ss: 0.104\n",
      "Epoch 935/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1038 - val_loss: 0.1050\n",
      "Epoch 936/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1050\n",
      "Epoch 937/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1050\n",
      "Epoch 938/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1022 - val_loss: 0.1050\n",
      "Epoch 939/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1050\n",
      "Epoch 940/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1050\n",
      "Epoch 941/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1003 - val_loss: 0.1050\n",
      "Epoch 942/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1050\n",
      "Epoch 943/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1050\n",
      "Epoch 944/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1011 - val_loss: 0.1049\n",
      "Epoch 945/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0998 - val_loss: 0.1049\n",
      "Epoch 946/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1049\n",
      "Epoch 947/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1049\n",
      "Epoch 948/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1021 - val_loss: 0.1049\n",
      "Epoch 949/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0980 - val_loss: 0.1049\n",
      "Epoch 950/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1004 - val_loss: 0.1049\n",
      "Epoch 951/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1049\n",
      "Epoch 952/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1049\n",
      "Epoch 953/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1042 - val_loss: 0.1049\n",
      "Epoch 954/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1020 - val_loss: 0.1049\n",
      "Epoch 955/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1048\n",
      "Epoch 956/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1048\n",
      "Epoch 957/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1048\n",
      "Epoch 958/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1048\n",
      "Epoch 959/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 960/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1048\n",
      "Epoch 961/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1048\n",
      "Epoch 962/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1029 - val_loss: 0.1048\n",
      "Epoch 963/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1048\n",
      "Epoch 964/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0979 - val_loss: 0.1048\n",
      "Epoch 965/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1048\n",
      "Epoch 966/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1047\n",
      "Epoch 967/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0998 - val_loss: 0.1047\n",
      "Epoch 968/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1017 - val_loss: 0.1047\n",
      "Epoch 969/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1047\n",
      "Epoch 970/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1047\n",
      "Epoch 971/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0989 - val_loss: 0.1047\n",
      "Epoch 972/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1047\n",
      "Epoch 973/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1016 - val_loss: 0.1047\n",
      "Epoch 974/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1047\n",
      "Epoch 975/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1003 - val_loss: 0.1047\n",
      "Epoch 976/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1047\n",
      "Epoch 977/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1047 - val_loss: 0.1046\n",
      "Epoch 978/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1004 - val_loss: 0.1046\n",
      "Epoch 979/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1039 - val_loss: 0.1046\n",
      "Epoch 980/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0996 - val_loss: 0.1046\n",
      "Epoch 981/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1046\n",
      "Epoch 982/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1013 - val_loss: 0.1046\n",
      "Epoch 983/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1010 - val_loss: 0.1046\n",
      "Epoch 984/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1006 - val_loss: 0.1046\n",
      "Epoch 985/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1041 - val_loss: 0.1046\n",
      "Epoch 986/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0985 - val_loss: 0.1046\n",
      "Epoch 987/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1002 - val_loss: 0.1046\n",
      "Epoch 988/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1046\n",
      "Epoch 989/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1027 - val_loss: 0.1045\n",
      "Epoch 990/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1002 - val_loss: 0.1045\n",
      "Epoch 991/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1045\n",
      "Epoch 992/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1015 - val_loss: 0.1045\n",
      "Epoch 993/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0989 - val_loss: 0.1045\n",
      "Epoch 994/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1006 - val_loss: 0.1045\n",
      "Epoch 995/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1019 - val_loss: 0.1045ss: 0.104\n",
      "Epoch 996/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1006 - val_loss: 0.1045\n",
      "Epoch 997/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1014 - val_loss: 0.1045\n",
      "Epoch 998/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.0977 - val_loss: 0.1045\n",
      "Epoch 999/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1045\n",
      "Epoch 1000/1000\n",
      "2234/2234 [==============================] - 0s - loss: 0.1009 - val_loss: 0.1044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121cf8dd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrain R\n",
    "make_trainable(R, True)\n",
    "make_trainable(D, False)\n",
    "# Train with noly signal\n",
    "#DfR.fit(X_train[y_train==1], mass_train[y_train==1], sample_weight=weights_train[y_train==1], nb_epoch=5) # only signal\n",
    "#DfR.fit([X_train[y_train==1], X_train[y_train==1]], [mass_train[y_train==1],mass_train[y_train==1]], sample_weight=[weights_train[y_train==0],weights_train[y_train==1]], nb_epoch=5) # only signal\n",
    "# Train with noly signal, mask with weights=0\n",
    "DfR.fit(X_train, mass_train, sample_weight=weights_train*y_train, \n",
    "        callbacks=[tensorboard],\n",
    "        validation_data=(X_train, mass_train, weights_train*y_train), \n",
    "        nb_epoch=1000 )##15)#5)\n",
    "##DfR.fit(X_train, mass_train, sample_weight=weights_train*y_train, nb_epoch=1000)##15)#5)\n",
    "#DfR.fit(X_train, mass_train, sample_weight=weights_train, nb_epoch=5)\n",
    "\n",
    "#DfR.fit(X_train, mass_train, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.optimizers import adam\n",
    "#DfR.compile(loss=[make_loss_R(c=lam)], optimizer=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DfR.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DfR.evaluate(X_test, [mass_test], sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_losses(i, losses):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    ax1 = plt.subplot(311)   \n",
    "    values = np.array(losses[\"L_f\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f$\", color=\"blue\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax2 = plt.subplot(312, sharex=ax1) \n",
    "    values = np.array(losses[\"L_r\"]) #/ lam\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_r$\", color=\"green\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax3 = plt.subplot(313, sharex=ax1)\n",
    "    values = np.array(losses[\"L_f - L_r\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f - \\lambda L_r$\", color=\"red\")  \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "losses = {\"L_f\": [], \"L_r\": [], \"L_f - L_r\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 128\n",
    "training_iterations = 50#201\n",
    "for i in range(training_iterations):\n",
    "    l = DRf.evaluate(X_test, [y_test, mass_test], sample_weight=[weights_test,weights_test], verbose=0)\n",
    "    losses[\"L_f - L_r\"].append(l[0][None][0])\n",
    "    losses[\"L_f\"].append(l[1][None][0]) # why none, 0? just do l[1]??\n",
    "    losses[\"L_r\"].append(-l[2][None][0])\n",
    "    print(losses[\"L_f\"][-1], losses[\"L_r\"][-1] / lam, losses[\"L_r\"][-1])\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        plot_losses(i, losses)\n",
    "\n",
    "    # Fit D\n",
    "    make_trainable(R, False)\n",
    "    make_trainable(D, True)\n",
    "    indices = np.random.permutation(len(X_train))[:batch_size]\n",
    "    print \"DRf\"\n",
    "    DRf.train_on_batch(X_train[indices], [y_train[indices], mass_train[indices]], sample_weight=[weights_train[indices], weights_train[indices]])\n",
    "        \n",
    "    # Fit R\n",
    "    make_trainable(R, True)\n",
    "    make_trainable(D, False)\n",
    "    print \"DfR\"\n",
    "    DfR.fit(X_train, mass_train, batch_size=batch_size, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #@TODO: try grad reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8480996343305256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_dc = D.predict(X_test)\n",
    "y_pred_dc = y_pred_dc.ravel()\n",
    "roc_auc_score(y_test, y_pred_dc, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(y_pred_dc[mass_test<mass_test.mean()], weights=weights_test[mass_test<mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred_dc[mass_test>=mass_test.mean()], weights=weights_test[mass_test>=mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted correlation with mass is (0.13832442, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr = pearsonr(mass_test, y_pred_dc)\n",
    "print \"Unweighted correlation with mass is\", corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8391127 ,  0.34017679,  0.32018641, ...,  0.35115641,\n",
       "        0.97720736,  0.21300398], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict = R.predict(X_test)\n",
    "mass_predict_train = R.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02630521],\n",
       "       [ 0.01702765],\n",
       "       [ 0.01665593],\n",
       "       ..., \n",
       "       [ 0.01723181],\n",
       "       [ 0.02887303],\n",
       "       [ 0.01466291]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean true 0.192913 mean pred 0.0231963\n"
     ]
    }
   ],
   "source": [
    "print \"mean true\",(mass_test[y_test==1]).mean(), \"mean pred\", np.mean(mass_predict[y_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median true 0.161727428436 median pred 0.0254936\n"
     ]
    }
   ],
   "source": [
    "print \"median true\",(mass_test[y_test==1]).median(), \"median pred\", np.median(mass_predict[y_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std true 0.146328 std pred 0.00593165\n"
     ]
    }
   ],
   "source": [
    "print \"std true\",(mass_test[y_test==1]).std(), \"std pred\", np.std(mass_predict[y_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: mean true 0.189247 mean pred 0.0239481\n"
     ]
    }
   ],
   "source": [
    "print \"Train: mean true\",(mass_train[y_train==1]).mean(), \"mean pred\", np.mean(mass_predict_train[y_train==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: median true 0.16191893816 median pred 0.0259343\n"
     ]
    }
   ],
   "source": [
    "print \"Train: median true\",(mass_train[y_train==1]).median(), \"median pred\", np.median(mass_predict_train[y_train==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: std true 0.138881 std pred 0.00532954\n"
     ]
    }
   ],
   "source": [
    "print \"Train: std true\",(mass_train[y_train==1]).std(), \"std pred\", np.std(mass_predict_train[y_train==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220480/221190 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.7734772806553148e-05"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DfR.evaluate(X_test, [mass_test], sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8PO3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120a79d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mass_predict, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "plt.hist(mass_test, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "\n",
    "#plt.ylim(0, 5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#doesnt change after adversrial training.. should get worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3P8HxVe/+BD/8YYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12121b7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mass_predict[y_test==1], weights=weights_test[y_test==1], bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "plt.hist(mass_test[y_test==1], weights=weights_test[y_test==1], bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "\n",
    "#plt.ylim(plt.titlelt.title(\"VBF Events\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFl9JREFUeJzt3X+QVOWd7/H3F9CgDDu4qJQFBtiI\nWgqiDBooYgySlBi9GLMx6ibRbGHID71urZso3r1VYe/VCqwpjVtLzLqrEW4qTrJWTAiJWaLAWjFF\nvLpLFEFlyGXjEDdGFgiTgAZ87h9zxAYHprune3r6mferqsvT5zx9zqePzWfOnO4+EyklJEn5GtLo\nAJKk+rLoJSlzFr0kZc6il6TMWfSSlDmLXpIyZ9FLUuYseknKnEUvSZkb1ugAAMcff3yaMGFCVY/9\n3e9+x4gRI2obqI6aLS80X2bz1l+zZc4179NPP/1qSumEXgemlBp+a2trS9Vas2ZN1Y9thGbLm1Lz\nZTZv/TVb5lzzAk+lMjrWUzeSlDmLXpIyZ9FLUuYGxJuxkvL0hz/8gc7OTvbu3dvQHK2trWzatKmh\nGSpxaN7hw4czbtw4jjrqqKrWZ9FLqpvOzk5GjhzJhAkTiIiG5di9ezcjR45s2PYrVZo3pcT27dvp\n7Oxk4sSJVa3PUzeS6mbv3r2MHj26oSXf7CKC0aNH9+m3IoteUl1Z8n3X131o0UtS5jxHL6nfzFq8\nmm0799RsfWNHHcMTCy887PKhQ4cyZcoUXn/9dc4880yWLVvGscceW9W21q5dy5e//GVWrlzJihUr\n2LhxIwsXLuxx7M6dO/nmN7/J5z73uYq2sWjRIlpaWvj0pz9dVcbDGZRFP2HhD9i6+JJGx5AGnW07\n99T0396EhT844vJjjjmG9evXs3v3bj7zmc/wta99jZtuuunA8gPfHB1S2cmNefPmMW/evMMu37lz\nJ1/96lcrLvp68dSNpEHh/PPPp6Ojg61bt3LaaadxzTXXMHnyZF566SVWrVrFzJkzmTZtGldccQVd\nXV0A/OhHP+L0009n2rRpfOc73zmwrgceeIAbbrgBgF//+tdcfvnlTJ06lalTp/LTn/6UhQsXsmXL\nFs4++2y+8IUvAHDHHXdw7rnnctZZZ/HFL37xwLpuv/12Tj31VN7znvfwwgsv1OW5D8ojekmDy759\n+3jkkUeYO3cuAJs3b2bZsmXMmDGDV199ldtuu41HH32UESNGsGTJEu68805uvvlmPvWpT7F69WpO\nOeUUrrzyyh7XfeONN3LBBRfw8MMPs3//frq6uli8eDEbNmxg/fr1AKxatYrNmzfz5JNPklJi3rx5\nPP7444wYMYL29nbWr1/Pvn37mDZtGm1tbTV//ha9pGzt2bOHs88+mzfeeIMLLriA+fPn86tf/Yrx\n48czY8YMANatW8fGjRuZNWsWAK+//jozZ87k+eefZ+LEiUyaNAmAj3/849x7771v28bq1atZvnw5\n0P2eQGtrKzt27DhozKpVq1i1ahXnnHMOAF1dXWzevJndu3dz+eWXH3jf4Eing/rCopeUrdJz9KVf\nmCq9BHBKiQ984AM8+OCDBz32zaPxWkgpceutt77tTdavfOUrNdvGkXiOXtKgNmPGDJ544gk6OjqA\n7mvBv/jii5x++uls3bqVLVu2ALztB8Gb5syZwz333APA/v372bVrFyNHjmT37t0Hxlx00UXcf//9\nB879b9u2jVdeeYX3vve9fPe732XPnj3s3r2b73//+3V5jh7RS+o3Y0cd0+snZSpdX1+dcMIJPPDA\nA1x99dW89tprANx2222ceuqp3HvvvVxyySUce+yxnH/++QeV95vuvvtuFixYwH333cfQoUO55557\nmDlzJrNmzWLy5MlcfPHF3HHHHWzatImZM2cC0NLSwje+8Q2mTZvGlVdeydSpUznxxBM599xz+/x8\nelTORevrfevvPzwy/paVVW+vr5rtDyCk1HyZzVt/5WbeuHFjfYOU6be//W2jI1Skp7w97Uv8wyOS\nJPAcvSRlz6KXpMxZ9JKUOYtekjJn0UtS5vwcvaT+c9cU2PXL2q2v9Z3wl8/2uGj79u3MmTMHgJdf\nfplhw4ZxwgknAPDkk09y9NFH1y7HAGfRS+o/u34Ji3bVbn2LWg+7aPTo0QcuY3DrrbcyevRoPv/5\nzx805sDnzCu8THGzyfvZSdIhOjo6OOOMM/jYxz7GmWeeyUsvvcSoUaMOLG9vb+e6664Dui9B/OEP\nf5jp06dz3nnnsW7dukbF7hOP6CUNOs8//zzLly9n+vTp7Nu377DjbrzxRm6++WZmzJjB1q1bufTS\nS9mwYUM/Jq0Ni17SoPOud72L6dOn9zru0UcfPeiPgezYsYM9e/ZwzDF9v8ZOfyq76CNiKPAUsC2l\ndGlETATagdHA08AnUkqvR8Q7gOVAG7AduDKltLXmySWpSqWXKR4yZAjdl43ptnfv3gPTKaUs3rit\n5Bz9XwCbSu4vAe5KKZ0C7ADmF/PnAzuK+XcV4yRpQBoyZAjHHXccmzdv5o033uDhhx8+sOz9738/\nS5cuPXC/lteo709lHdFHxDjgEuB24KaICOBC4M+KIcuARcA9wGXFNMBDwN9HRKTSH5mSBqfWdx7x\nkzJVra8GlixZwkUXXcSJJ55IW1vbgcsVL126lM9+9rN8/etfZ9++fcyePfug4m8WUU7/RsRDwJeA\nkcDngU8C64qjdiLiZOCRlNLkiNgAzE0pdRbLtgDvTim9esg6FwALAMaMGdPW3t5e1RPo6uqipaWl\nosc8u20XU8bW8MVWgWryNlqzZTZv/ZWbubW1lVNOOaUfEh3Z/v37GTp0aKNjlK2nvB0dHezadfBH\nU2fPnv10Sqn3Nxt6u44xcCnw1WL6fcBK4Higo2TMycCGYnoDMK5k2Rbg+CNtw+vRD2zNltm89ef1\n6Our1tejL+fUzSxgXkR8EBgO/BFwNzAqIoallPYB44BtxfhtRfF3RsQwoJXuN2UlSQ3Q65uxKaVb\nU0rjUkoTgKuA1SmljwFrgI8Uw64FvldMryjuUyxfXfzkkTQI+c+/7/q6D/vyzdhb6H5jtoPuj1je\nV8y/DxhdzL8JWNinhJKa1vDhw9m+fbtl3wcpJbZv387w4cOrXkdFX5hKKa0F1hbTvwDO62HMXuCK\nqhNJysa4cePo7OzkN7/5TUNz7N27t09F2d8OzTt8+HDGjRtX9fr8ZqykujnqqKOYOHFio2Owdu1a\nzjnnnEbHKFut83pRM0nKnEUvSZmz6CUpcxa9JGXOopekzFn0kpQ5i16SMmfRS1LmLHpJypxFL0mZ\ns+glKXMWvSRlzqKXpMxZ9JKUOYtekjJn0UtS5ix6ScqcRS9JmbPoJSlzFr0kZc6il6TMWfSSlDmL\nXpIyZ9FLUuYseknKnEUvSZmz6CUpcxa9JGXOopekzFn0kpQ5i16SMmfRS1LmLHpJypxFL0mZs+gl\nKXMWvSRlrteij4jhEfFkRPw8Ip6LiL8p5k+MiJ9FREdEfCsiji7mv6O431Esn1DfpyBJOpJyjuhf\nAy5MKU0FzgbmRsQMYAlwV0rpFGAHML8YPx/YUcy/qxgnSWqQXos+desq7h5V3BJwIfBQMX8Z8KFi\n+rLiPsXyORERNUssSapIpJR6HxQxFHgaOAVYCtwBrCuO2omIk4FHUkqTI2IDMDel1Fks2wK8O6X0\n6iHrXAAsABgzZkxbe3t7VU+gq6uLlpaWih7z7LZdTBnbWtX2+qqavI3WbJnNW3/NljnXvLNnz346\npTS914EppbJvwChgDfAeoKNk/snAhmJ6AzCuZNkW4PgjrbetrS1Va82aNRU/ZvwtK6veXl9Vk7fR\nmi2zeeuv2TLnmhd4KpXR3RV96ialtLMo+pnAqIgYViwaB2wrprcVxU+xvBXYXsl2JEm1U86nbk6I\niFHF9DHAB4BNdBf+R4ph1wLfK6ZXFPcplq8ufvJIkhpgWO9DOAlYVpynHwJ8O6W0MiI2Au0RcRvw\n78B9xfj7gP8TER3AfwFX1SG3JKlMvRZ9SukZ4Jwe5v8COK+H+XuBK2qSTpLUZ34zVpIyZ9FLUuYs\neknKnEUvSZmz6CUpcxa9JGXOopekzFn0kpQ5i16SMmfRS1LmLHpJypxFL0mZs+glKXMWvSRlzqKX\npMxZ9JKUOYtekjJn0UtS5ix6ScqcRS9JmbPoJSlzFr0kZc6il6TMWfSSlDmLXpIyZ9FLUuYseknK\nnEUvSZmz6CUpcxa9JGXOopekzFn0kpQ5i16SMmfRS1LmLHpJypxFL0mZs+glKXO9Fn1EnBwRayJi\nY0Q8FxF/Ucz/44j4cURsLv57XDE/IuLvIqIjIp6JiGn1fhKSpMMr54h+H/BXKaUzgBnA9RFxBrAQ\neCylNAl4rLgPcDEwqbgtAO6peWpJUtl6LfqU0ssppX8rpncDm4CxwGXAsmLYMuBDxfRlwPLUbR0w\nKiJOqnlySVJZKjpHHxETgHOAnwFjUkovF4v+ExhTTI8FXip5WGcxT5LUAJFSKm9gRAvwr8DtKaXv\nRMTOlNKokuU7UkrHRcRKYHFK6SfF/MeAW1JKTx2yvgV0n9phzJgxbe3t7VU9ga6uLlpaWip6zLPb\ndjFlbGtV2+uravI2WrNlNm/9NVvmXPPOnj376ZTS9F4HppR6vQFHAf8C3FQy7wXgpGL6JOCFYvof\ngKt7Gne4W1tbW6rWmjVrKn7M+FtWVr29vqomb6M1W2bz1l+zZc41L/BUKqPDy/nUTQD3AZtSSneW\nLFoBXFtMXwt8r2T+NcWnb2YAu9Jbp3gkSf1sWBljZgGfAJ6NiPXFvP8BLAa+HRHzgf8APlos+yHw\nQaAD+D3w5zVNLEmqSK9Fn7rPtcdhFs/pYXwCru9jLklSjfjNWEnKnEUvSZmz6CUpcxa9JGXOopek\nzFn0kpQ5i16SMmfRS1LmLHpJypxFL0mZs+glKXMWvSRlzqKXpMxZ9JKUOYtekjJn0UtS5ix6Scqc\nRS9JmbPoJSlzFr0kZc6il6TMWfSSlDmLXpIyZ9FLUuYseknKnEUvSZmz6CUpcxa9JGXOopekzFn0\nkpQ5i16SMjfoin7W4tWNjiBJ/WrQFf22nXsaHUGS+tWgK3pJGmwseknKnEUvSZmz6CUpc70WfUTc\nHxGvRMSGknl/HBE/jojNxX+PK+ZHRPxdRHRExDMRMa2e4SVJvSvniP4BYO4h8xYCj6WUJgGPFfcB\nLgYmFbcFwD21iSlJqlavRZ9Sehz4r0NmXwYsK6aXAR8qmb88dVsHjIqIk2oVVpJUuWrP0Y9JKb1c\nTP8nMKaYHgu8VDKus5gnSWqQSCn1PihiArAypTS5uL8zpTSqZPmOlNJxEbESWJxS+kkx/zHglpTS\nUz2scwHdp3cYM2ZMW3t7e1VPoKuri5aWlrLHP7ttFwBTxrZWtb2+qjTvQNBsmc1bf82WOde8s2fP\nfjqlNL3XgSmlXm/ABGBDyf0XgJOK6ZOAF4rpfwCu7mnckW5tbW2pWmvWrKlo/PhbVqbxt6ysent9\nVWnegaDZMpu3/potc655gadSGR1e7ambFcC1xfS1wPdK5l9TfPpmBrArvXWKR5LUAMN6GxARDwLv\nA46PiE7gi8Bi4NsRMR/4D+CjxfAfAh8EOoDfA39eh8ySpAr0WvQppasPs2hOD2MTcH1fQ0mSasdv\nxkpS5ix6ScqcRS9JmbPoJSlzFr0kZc6il6TMWfSSlDmLXpIyZ9FLUuYseknK3KAq+lmLVzc6giT1\nu0FV9Nt27ml0BEnqd4Oq6CVpMLLoJSlzFr0kZc6il6TMDYqin7DwB42OIEkNMyiKXpIGM4tekjJn\n0UtS5gZN0XueXtJgNWiKXpIGq2yL/nBH8FsXX9LPSSSpsbIteklSt6yL3vPykpRp0ZdejtiylzTY\nZVn0Xo5Ykt6SZdFLkt4yqIreT9xIGowGTdFb8pIGq0FR9GNHHdPoCJLUMIOi6J9YeGGjI0hSwwyK\nopekwcyil6TMZVv0vvkqSd2yLXpJUrdhjQ5QTx7VS1KdjugjYm5EvBARHRGxsB7bkCSVp+ZFHxFD\ngaXAxcAZwNURcUattyNJKk89Tt2cB3SklH4BEBHtwGXAxjpsC17ZCIsuO2jW1uHAosOMb30nsLjn\nZXdNgV2/7Pkxf/ls+eOP5LS/eVveih0uTy2VPrfSzLXadqX7WlLV6lH0Y4GXSu53Au+uw3a67X8d\nFu06cPfNyxIf9vz8XVPYOvzPev5B0PrOg9ZV+hgWtZY//kjWroWrK3xMuXlqqfS5lWau1bYr3deV\nqMUP0/50xpeA9719fqUHErU6IPGHbXYipVTbFUZ8BJibUrquuP8J4N0ppRsOGbcAWFDcPQ14ocpN\nHg+8WuVjG6HZ8kLzZTZv/TVb5lzzjk8pndDboHoc0W8DTi65P66Yd5CU0r3AvX3dWEQ8lVKa3tf1\n9JdmywvNl9m89ddsmQd73np86ub/ApMiYmJEHA1cBayow3YkSWWo+RF9SmlfRNwA/AswFLg/pfRc\nrbcjSSpPXb4wlVL6IfDDeqy7B30+/dPPmi0vNF9m89Zfs2Ue1Hlr/masJGlg8Vo3kpS5pin63i6r\nEBHviIhvFct/FhET+j/lQXl6y/veiPi3iNhXfCS1ocrIe1NEbIyIZyLisYgY34ich2TqLfNnIuLZ\niFgfET9p9De0y700SET8aUSkiGjop0TK2L+fjIjfFPt3fURc14ich2TqdR9HxEeL1/JzEfHN/s54\nSJbe9vFdJfv3xYjYWdWGUkoD/kb3m7pbgD8BjgZ+DpxxyJjPAV8rpq8CvjXA804AzgKWAx9pgv07\nGzi2mP5sI/dvBZn/qGR6HvCjgZy3GDcSeBxYB0wfyHmBTwJ/38jXQRWZJwH/DhxX3D9xIOc9ZPx/\np/vDLRVvq1mO6A9cViGl9Drw5mUVSl0GLCumHwLmRET0Y8ZSveZNKW1NKT0DvNGIgIcoJ++alNLv\ni7vr6P5+RCOVk/m3JXdHAI18Q6qc1zDA/waWAHv7M1wPys07kJST+VPA0pTSDoCU0iv9nLFUpfv4\nauDBajbULEXf02UVxh5uTEppH7ALGN0v6d6unLwDSaV55wOP1DVR78rKHBHXR8QW4G+BG/spW096\nzRsR04CTU0o/6M9gh1Hua+JPi9N5D0XEyT0s70/lZD4VODUinoiIdRExt9/SvV3Z/+6KU6UTgdXV\nbKhZil4DRER8HJgO3NHoLOVIKS1NKb0LuAX4n43OczgRMQS4E/irRmepwPeBCSmls4Af89Zv1APZ\nMLpP37yP7iPkf4yIUQ1NVJ6rgIdSSvureXCzFH05l1U4MCYihgGtwPZ+Sfd2ZV0GYgApK29EvB/4\na2BeSum1fsp2OJXu43bgQ3VNdGS95R0JTAbWRsRWYAawooFvyPa6f1NK20teB/8EtPVTtsMp5zXR\nCaxIKf0hpfT/gBfpLv5GqOQ1fBVVnrYBmubN2GHAL+j+1eXNNy3OPGTM9Rz8Zuy3B3LekrEP0Pg3\nY8vZv+fQ/cbRpEa/HirIPKlk+r8BTw3kvIeMX0tj34wtZ/+eVDJ9ObCuCV4Tc4FlxfTxdJ86GT1Q\n8xbjTge2UnzvqaptNfJ/TIU75YN0//TdAvx1Me9/0X10CTAc+GegA3gS+JMBnvdcuo8ufkf3bx7P\nDfC8jwK/BtYXtxVN8Jq4G3iuyLvmSMU6EPIeMrahRV/m/v1SsX9/Xuzf05vgNRF0nyLbCDwLXDWQ\n8xb3FwGL+7IdvxkrSZlrlnP0kqQqWfSSlDmLXpIyZ9FLUuYseknKnEUvSZmz6CUpcxa9JGXu/wMc\nt7DOkQMbZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12121bc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mass_predict_train[y_train==1], weights=weights_train[y_train==1], bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "plt.hist(mass_train[y_train==1], weights=weights_train[y_train==1], bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "\n",
    "#plt.ylim(plt.titlelt.title(\"VBF Events\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why cant it memorise? are there identical events? need to pca and project to 3d then see\n",
    "# mass comparison.. maybe binned regression is better with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dc_train = D.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_pred_test_sig = [weights_train[(y_train ==1) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "#int_pred_test_bkg = [weights_train[(y_train ==0) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "int_pred_dc_test_sig = [weights_test[(y_test ==1) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_dc_test_bkg = [weights_test[(y_test ==0) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1215698d0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120a85790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_sig)\n",
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import amsasimov\n",
    "vamsasimov_dc = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_dc_test_sig,int_pred_dc_test_bkg)]\n",
    "significance_dc = max(vamsasimov_dc)\n",
    "threshold_dc = np.linspace(0,1,num=50)[ np.array(vamsasimov_dc).argmax() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_dc, threshold_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import compare_train_test\n",
    "compare_train_test(y_pred_dc_train, y_train, y_pred_dc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGevMass(mass):\n",
    "    return np.exp (mass * (mass_max - mass_min) + mass_min)\n",
    "plt.hist(getGevMass(mass_test), weights=weights_test, bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score > \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Mass Distribution (s+b)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = np.zeros((len(mass_test),), dtype=[('mass',np.float64),('weight',np.float64),('NN_score',np.float64) ])\n",
    "temp['mass'] = np.array(getGevMass(mass_test))\n",
    "temp['weight'] = np.array(weights_test)\n",
    "temp['NN_score'] = np.array(y_pred_dc)\n",
    "\n",
    "from root_numpy import array2tree\n",
    "tree_dc = array2tree(temp)\n",
    "\n",
    "from ROOT import TEfficiency, TH1F\n",
    "bins = 50\n",
    "scoremin = temp['mass'].min()\n",
    "scoremax = temp['mass'].max()\n",
    "hpreselect_dc = TH1F(\"hpreselect_dc\", \"mass distribution before NN\", bins, scoremin, scoremax)\n",
    "hpreselect_dc.Sumw2()\n",
    "hD = TH1F(\"hD\", \"mass distribution for D Score > \" + str(threshold_dc), bins, scoremin, scoremax)\n",
    "hD.Sumw2()\n",
    "#tree.Project(\"hpreselect\", \"mass\", \"weight\" ) #Tefficiency can;t do weights\n",
    "tree_dc.Project(\"hpreselect_dc\", \"mass\" )\n",
    "#tree.Project(\"hNN\", \"mass\", \"weight*(NN_score>=\" +str(threshold) + \")\" )\n",
    "tree_dc.Project(\"hD\", \"mass\", \"(NN_score>=\" +str(threshold_dc) + \")\" )\n",
    "\n",
    "print TEfficiency.CheckConsistency(hD, hpreselect_dc)\n",
    "pEff_dc = TEfficiency(hD, hpreselect_dc)\n",
    "\n",
    "from ROOT import TCanvas\n",
    "c_dc = TCanvas(\"dcCanvas\",\"dc Canvas\",800,350)\n",
    "pEff_dc.SetTitle(\"Efficiency: Pre-selection vs Post D Selection;Mass (GeV) ;#epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pEff_dc.Draw(\"AP\")\n",
    "#ROOT.enableJSVis()\n",
    "c_dc.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mass_predict !=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again with adverserial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(i, losses):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    ax1 = plt.subplot(311)   \n",
    "    values = np.array(losses[\"L_f\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f$\", color=\"blue\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax2 = plt.subplot(312, sharex=ax1) \n",
    "    values = np.array(losses[\"L_r\"]) #/ lam\n",
    "    plt.plot(range(len(values)), values, label=r\"$\\lambda L_r$\", color=\"green\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax3 = plt.subplot(313, sharex=ax1)\n",
    "    values = np.array(losses[\"L_f - L_r\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f - \\lambda L_r$\", color=\"red\")  \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"L_f\": [], \"L_r\": [], \"L_f - L_r\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "training_iterations = 201 #50\n",
    "for i in range(training_iterations):\n",
    "    #l = DRf.evaluate(X_test, [y_test, mass_test], sample_weight=[weights_test,weights_test], verbose=0)  \n",
    "    l = DRf.evaluate(X_train, [y_train, mass_train], sample_weight=[weights_train,weights_train], verbose=0)  \n",
    "    losses[\"L_f - L_r\"].append(l[0][None][0])\n",
    "    losses[\"L_f\"].append(l[1][None][0]) # why none, 0? just do l[1]??\n",
    "    losses[\"L_r\"].append(-l[2][None][0]) # the - cancels the - in loss -lam\n",
    "    print(losses[\"L_f\"][-1], losses[\"L_r\"][-1] / lam, losses[\"L_r\"][-1])\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        plot_losses(i, losses)\n",
    "\n",
    "    # Fit D\n",
    "    make_trainable(R, False)\n",
    "    make_trainable(D, True)\n",
    "    indices = np.random.permutation(len(X_train))[:batch_size]\n",
    "    print \"DRf\"\n",
    "    #use iloc\n",
    "    ##DRf.train_on_batch(X_train[indices], [y_train[indices], mass_train[indices]], sample_weight=[weights_train[indices], weights_train[indices]])\n",
    "    DRf.train_on_batch(X_train[indices], [y_train[indices], mass_train[indices]], sample_weight=[weights_train[indices], weights_train[indices]*y_train[indices]])\n",
    "    #@TODO: Make explicite masking with weights, y_train mught become multiclass later\n",
    "    \n",
    "    # Fit R\n",
    "    make_trainable(R, True)\n",
    "    make_trainable(D, False)\n",
    "    print \"DfR\"\n",
    "    #masking background events\n",
    "    DfR.fit(X_train, mass_train, batch_size=batch_size, sample_weight=weights_train*y_train, nb_epoch=1, verbose=1)\n",
    "    ##DfR.fit(X_train, mass_train, batch_size=batch_size, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #@TODO: try grad reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_dc = D.predict(X_test)\n",
    "y_pred_dc = y_pred_dc.ravel()\n",
    "roc_auc_score(y_test, y_pred_dc, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(y_pred_dc[mass_test<mass_test.mean()], weights=weights_test[mass_test<mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred_dc[mass_test>=mass_test.mean()], weights=weights_test[mass_test>=mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr = pearsonr(mass_test, y_pred_dc)\n",
    "print \"Unweighted correlation with mass is\", corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict = R.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DfR.evaluate(X_test, [mass_test], sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mass_predict, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "plt.hist(mass_test, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "\n",
    "#plt.ylim(0, 5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#doesnt change after adversrial training.. should get worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dc_train = D.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_pred_test_sig = [weights_train[(y_train ==1) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "#int_pred_test_bkg = [weights_train[(y_train ==0) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "int_pred_dc_test_sig = [weights_test[(y_test ==1) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_dc_test_bkg = [weights_test[(y_test ==0) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_sig)\n",
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import amsasimov\n",
    "vamsasimov_dc = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_dc_test_sig,int_pred_dc_test_bkg)]\n",
    "significance_dc = max(vamsasimov_dc)\n",
    "threshold_dc = np.linspace(0,1,num=50)[ np.array(vamsasimov_dc).argmax() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_dc, threshold_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import compare_train_test\n",
    "compare_train_test(y_pred_dc_train, y_train, y_pred_dc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGevMass(mass):\n",
    "    return np.exp (mass * (mass_max - mass_min) + mass_min)\n",
    "plt.hist(getGevMass(mass_test), weights=weights_test, bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Mass Distribution (s+b)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But R looks at all events, not just the ones with high score so it tries to \n",
    "#also predict the mass of background. It shouldnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test,y=y_pred_dc, weights=weights_test, bins=50, label=\"Preselection\")\n",
    "#plt.hist2d(x=getGevMass(mass_test),y=y_pred_dc, weights=weights_test, bins=50, normed=1, label=\"Preselection\")\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of all Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test[y_pred_dc >= threshold_dc],y=y_pred_dc[y_pred_dc >= threshold_dc], weights=weights_test[y_pred_dc >= threshold_dc], bins=50, label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of Score >= \" + str(threshold_dc)+\" Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test[y_test ==1],y=y_pred_dc[y_test ==1], weights=weights_test[y_test ==1], bins=50, label=\"True VBF Events\")\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of Score >= \" + str(threshold_dc)+\" Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test[y_pred_dc < threshold_dc],y=y_pred_dc[y_pred_dc < threshold_dc], weights=weights_test[y_pred_dc < threshold_dc], bins=50, label=\"Score < \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of Score < \" + str(threshold_dc)+\" Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((len(mass_test),), dtype=[('mass',np.float64),('weight',np.float64),('NN_score',np.float64) ])\n",
    "temp['mass'] = np.array(getGevMass(mass_test))\n",
    "temp['weight'] = np.array(weights_test)\n",
    "temp['NN_score'] = np.array(y_pred_dc)\n",
    "\n",
    "from root_numpy import array2tree\n",
    "tree_dc = array2tree(temp)\n",
    "\n",
    "from ROOT import TEfficiency, TH1F\n",
    "bins = 50\n",
    "scoremin = temp['mass'].min()\n",
    "scoremax = temp['mass'].max()\n",
    "hpreselect_dc = TH1F(\"hpreselect_dc\", \"mass distribution before NN\", bins, scoremin, scoremax)\n",
    "hpreselect_dc.Sumw2()\n",
    "hD = TH1F(\"hD\", \"mass distribution for D Score > \" + str(threshold_dc), bins, scoremin, scoremax)\n",
    "hD.Sumw2()\n",
    "#tree.Project(\"hpreselect\", \"mass\", \"weight\" ) #Tefficiency can;t do weights\n",
    "tree_dc.Project(\"hpreselect_dc\", \"mass\" )\n",
    "#tree.Project(\"hNN\", \"mass\", \"weight*(NN_score>=\" +str(threshold) + \")\" )\n",
    "tree_dc.Project(\"hD\", \"mass\", \"(NN_score>=\" +str(threshold_dc) + \")\" )\n",
    "\n",
    "print TEfficiency.CheckConsistency(hD, hpreselect_dc)\n",
    "pEff_dc = TEfficiency(hD, hpreselect_dc)\n",
    "\n",
    "from ROOT import TCanvas\n",
    "c_dc = TCanvas(\"dcCanvas\",\"dc Canvas\",800,350)\n",
    "pEff_dc.SetTitle(\"Efficiency: Pre-selection vs Post D Selection;Mass (GeV) ;#epsilon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pEff_dc.Draw(\"AP\")\n",
    "#ROOT.enableJSVis()\n",
    "c_dc.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mass_predict !=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard : PCA and see if its impossible to even memorize mass, identical events diff mass\n",
    "# find out why nan if I use indices but not train_test_split [because .iloc, fix]\n",
    "# try to just make a NN map 1 to 2, 2 to 6, 3 to 9 memorise it\n",
    "\n",
    "#: inject mass correlation and see if R learns anything at all. Find the elephant\n",
    "# R is sometimes learning somtimes not.. why.. reproducible problem?\n",
    "# the D without adverserial training is asmost same as after adverserial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sometimes training R gives 0 for every event... why. not reproducible?\n",
    "# why is the loss look decreasing further on pivot training, its not actually cuz auc is worse\n",
    "#@TODO: Compare AUC/significance with qq, without qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Victor:\n",
    "# pivot not working well\n",
    "# V: Try to overfit on 100 samples: find elephant problems\n",
    "# regression activation, loss function?\n",
    "# V: Its okay trial and error | andreas did classification\n",
    "# loss of r is changing at a lower level, maybe subtract off the first loss?\n",
    "# V: maybe\n",
    "# grad reversal layer\n",
    "# V: simultaneous could be better\n",
    "# why is the loss look decreasing further on pivot training, its not actually cuz auc is worse\n",
    "# V: Maybe you need to let pre-training converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip2 install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ams loss function: celver way to impliment, if else wont work: like makeing soft cuts\n",
    "# how close to 0, how close to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "635px",
    "left": "0px",
    "right": "1070px",
    "top": "110px",
    "width": "128px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
