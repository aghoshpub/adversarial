{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load signal, backgound data\n",
    "vbf_events = pd.read_hdf(\"../MC_Prod_v12/vbf_events.hdf\", \"vbf\") #do hdf5!!\n",
    "ggf_events = pd.read_hdf(\"../MC_Prod_v12/ggF_events.hdf\", \"ggF\")\n",
    "qq_events = pd.read_hdf(\"../MC_Prod_v12/qq_all_events.hdf\", \"qq_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbf_events[\"class\"] = 1\n",
    "ggf_events[\"class\"] = 2 # need to reweight ggF better! set to 0 afterwards\n",
    "qq_events[\"class\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60.047187373963304, 6.0885523004705586, 8.7771621167023159)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.047187373963304"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = qq_events.weight_couplings.sum(), vbf_events.weight_couplings.sum(), ggf_events.weight_couplings.sum()\n",
    "print class_weights\n",
    "max(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([vbf_events, ggf_events, qq_events])\n",
    "#data = pd.concat([vbf_events, ggf_events])\n",
    "#print data.isnull().values.any()\n",
    "#data.describe()\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True) #shuffle the events\n",
    "target = data[\"class\"]\n",
    "mass = data[\"m4l_fsr\"]\n",
    "weights = data[\"weight_couplings\"]\n",
    "del data[\"class\"]\n",
    "del data[\"m4l_fsr\"]\n",
    "del data[\"weight_couplings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = mass.apply(np.log)\n",
    "mass_max, mass_min = mass.max(), mass.min()\n",
    "mass = (mass - mass_min)/(mass_max - mass_min) #!!! save max, min values to file\n",
    "#mass.describe()\n",
    "#plt.hist(mass)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([74279, 71211, 41422, 21046,  9698,  3926,  1332,   407,    87,    16]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(mass, bins=10, range=(0,1), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_binned, mbins = pd.cut(x=mass, bins=10, right=True, retbins=True, include_lowest=True,\\\n",
    "                            labels=False)\n",
    "                            #labels=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.42790000e+04,   7.12110000e+04,   4.14220000e+04,\n",
       "          2.10460000e+04,   9.69800000e+03,   3.92600000e+03,\n",
       "          1.33200000e+03,   4.07000000e+02,   8.70000000e+01,\n",
       "          1.60000000e+01]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVLn/wNFyqhMkdezoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1156e58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mass, bins=10)\n",
    "plt.hist((mass_binned)/9, bins=mbins)\n",
    "plt.hist(mass, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import np_utils\n",
    "#np_cat = np_utils.to_categorical(mass_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_mass = pd.get_dummies(mass_binned)\n",
    "# Be Very Careful! the labels are numbers, change to numpy before calling Keras\n",
    "#Indexxing will be all wrong otherwise\n",
    "# use iloc not pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweight Events\n",
    "Training: 1000x everything, fraction 1/3 VBF 1/3 ggF 1/3 qq ~~0.5 VBF, 0.25 ggF, 0.25 qq~~\n",
    "\n",
    "Testing: Back to original VBF, ggF, qq sum of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test, mass_train, mass_test, weights_train, weights_test = \\\n",
    "    train_test_split(data, target, dummy_mass, weights, train_size=0.75)\n",
    "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
    "y_train, y_test, mass_train, mass_test, weights_train, weights_test = \\\n",
    " y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "    mass_train.reset_index(drop=True), mass_test.reset_index(drop=True), \\\n",
    "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class_weights_test = weights_test[y_test == 0].sum(), weights_test[y_test == 1].sum(), weights_test[y_test == 2].sum()\n",
    "scale_up = 1000.\n",
    "for i in xrange(3):\n",
    "    weights_train[y_train == i] *= scale_up*max(class_weights)/ class_weights[i]\n",
    "    weights_test[y_test == i] *= class_weights[i]/class_weights_test[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45046.261243873087, 45043.27316203493, 45115.672078546748)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum(), weights_train[y_train == 2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60.047187373963311, 6.0885523004705586, 8.7771621167023142)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_test[y_test == 0].sum(), weights_test[y_test == 1].sum(), weights_test[y_test == 2].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make ggF background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == 2] = 0\n",
    "y_test[y_test == 2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "#@TODO: check other activations in Andreas, Gilles pivot\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "Dx = Dense(32, activation=\"relu\")(inputs)\n",
    "Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "Dx = Dense(1, activation=\"sigmoid\")(Dx)\n",
    "D = Model(input=[inputs], output=[Dx])\n",
    "D.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_train *=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(weights_train ==0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_class_weights(y, smooth_factor=0):\n",
    "    \"\"\"\n",
    "    Returns the weights for each class based on the frequencies of the samples\n",
    "    :param smooth_factor: factor that smooths extremely uneven weights\n",
    "    :param y: list of true labels (the labels must be hashable)\n",
    "    :return: dictionary with the weight for each class\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    counter = Counter(y)\n",
    "\n",
    "    if smooth_factor > 0:\n",
    "        p = max(counter.values()) * smooth_factor\n",
    "        for k in counter.keys():\n",
    "            counter[k] += p\n",
    "\n",
    "    majority = max(counter.values())\n",
    "\n",
    "    return {cls: float(majority) / count for cls, count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=10)\n",
    "#D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=1) #short for testing purposes\n",
    "#D.fit(X_train, y_train, nb_epoch=10) #unweighted training\n",
    "#D.fit(X_train, y_train, nb_epoch=10, class_weight=get_class_weights(y_train)) #Only interclass weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_class_weights(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vbf_events), len(ggf_events), len(qq_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test ==2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = D.predict(X_test)\n",
    "y_pred = y_pred.ravel()\n",
    "roc_auc_score(y_true=y_test, y_score=y_pred, sample_weight=weights_test)\n",
    "#roc_auc_score(y_true=y_test, y_score=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = D.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_pred_test_sig = [weights_train[(y_train ==1) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "#int_pred_test_bkg = [weights_train[(y_train ==0) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "int_pred_test_sig = [weights_test[(y_test ==1) & (y_pred > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg = [weights_test[(y_test ==0) & (y_pred > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),int_pred_test_sig)\n",
    "plt.plot(np.linspace(0,1,num=50),int_pred_test_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import amsasimov\n",
    "vamsasimov = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig,int_pred_test_bkg)]\n",
    "significance = max(vamsasimov)\n",
    "threshold = np.linspace(0,1,num=50)[ np.array(vamsasimov).argmax() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import compare_train_test\n",
    "compare_train_test(y_pred_train, y_train, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mass_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(y_pred[mass_test<mass_test.mean()], weights=weights_test[mass_test<mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred[mass_test>=mass_test.mean()], weights=weights_test[mass_test>=mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "#plt.hist(y_pred[mass_test<mass.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "#plt.hist(y_pred[mass_test>=mass.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "\n",
    "plt.ylim(0, 5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()# @TODO: do sep for signal background, plot mass : full mass dist, vs after cut on bdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signal_low = list (set( np.where(y_test==1)[0] ) & set( np.where(mass_test<mass_test.mean())[0]))\n",
    "signal_high = list (set( np.where(y_test==1)[0] ) & set( np.where(mass_test>=mass_test.mean())[0]))\n",
    "\n",
    "plt.hist(y_pred[signal_low], weights=weights_test[signal_low], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred[signal_high], weights=weights_test[signal_high], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.title(\"Predicted scores for VBF events for low and high mass\")\n",
    "plt.ylim(0, 8)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bkg_low = list (set( np.where(y_test==0)[0] ) & set( np.where(mass_test<mass_test.mean())[0]))\n",
    "bkg_high = list (set( np.where(y_test==0)[0] ) & set( np.where(mass_test>=mass_test.mean())[0]))\n",
    "\n",
    "plt.hist(y_pred[bkg_low], weights=weights_test[bkg_low], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred[bkg_high], weights=weights_test[bkg_high], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.title(\"Predicted scores for background events for low and high mass\")\n",
    "plt.ylim(0, 4)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGevMass(mass):\n",
    "    return np.exp (mass * (mass_max - mass_min) + mass_min)\n",
    "plt.hist(getGevMass(mass_test), weights=weights_test, bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[y_pred >= threshold]), weights=weights_test[y_pred >= threshold], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score > \" + str(threshold))\n",
    "\n",
    "plt.title(\"Mass Distribution (s+b)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((len(mass_test),), dtype=[('mass',np.float64),('weight',np.float64),('NN_score',np.float64) ])\n",
    "temp['mass'] = np.array(getGevMass(mass_test))\n",
    "temp['weight'] = np.array(weights_test)\n",
    "temp['NN_score'] = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from root_numpy import array2tree\n",
    "tree = array2tree(temp)\n",
    "\n",
    "from ROOT import TEfficiency, TH1F\n",
    "bins = 50\n",
    "scoremin = temp['mass'].min()\n",
    "scoremax = temp['mass'].max()\n",
    "hpreselect = TH1F(\"hpreselect\", \"mass distribution before NN\", bins, scoremin, scoremax)\n",
    "hpreselect.Sumw2()\n",
    "hNN = TH1F(\"hNN\", \"mass distribution for NN Score > \" + str(threshold), bins, scoremin, scoremax)\n",
    "hNN.Sumw2()\n",
    "#tree.Project(\"hpreselect\", \"mass\", \"weight\" ) #Tefficiency can;t do weights\n",
    "tree.Project(\"hpreselect\", \"mass\" )\n",
    "#tree.Project(\"hNN\", \"mass\", \"weight*(NN_score>=\" +str(threshold) + \")\" )\n",
    "tree.Project(\"hNN\", \"mass\", \"(NN_score>=\" +str(threshold) + \")\" )\n",
    "\n",
    "print TEfficiency.CheckConsistency(hNN, hpreselect)\n",
    "pEff = TEfficiency(hNN, hpreselect)\n",
    "\n",
    "from ROOT import TCanvas\n",
    "c = TCanvas(\"myCanvasName\",\"The Canvas Title\",800,350)\n",
    "pEff.SetTitle(\"Efficiency: Pre-selection vs Post NN Selection;Mass (GeV) ;#epsilon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pEff.Draw(\"AP\")\n",
    "#ROOT.enableJSVis()\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(getGevMass(mass_test[y_test==1]), weights=weights_test[y_test==1], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[(y_test==1) & (y_pred >= threshold)]), weights=weights_test[(y_test==1) & (y_pred >= threshold)], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(round(threshold,2)))\n",
    "\n",
    "plt.title(\"VBF Events\")\n",
    "plt.xlabel(\"Mass (GeV)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(220,800 )\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(getGevMass(mass_test[y_test==0]), weights=weights_test[y_test==0], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[(y_test==0) & (y_pred >= threshold)]), weights=weights_test[(y_test==0) & (y_pred >= threshold)], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(round(threshold,2)))\n",
    "\n",
    "plt.title(\"Background Like Events (qq, ggF)\")\n",
    "plt.xlim(220,800 )\n",
    "plt.xlabel(\"Mass (GeV)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr = pearsonr(mass_test, y_pred)\n",
    "print \"Unweighted correlation of all test events with mass is\", corr\n",
    "\n",
    "corr = pearsonr(mass_test[y_test ==1], y_pred[y_test ==1])\n",
    "print \"Unweighted correlation of signal test with mass is\", corr\n",
    "\n",
    "corr = pearsonr(mass_test[(y_pred > threshold) ], y_pred[(y_pred > threshold)])\n",
    "print \"Unweighted correlation of all test events passing cut with mass is\", corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(y_pred>0.5).sum()/float(y_pred.shape[0]) # much better than without class_weight training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def make_trainable(network, flag):\n",
    "    network.trainable = flag\n",
    "    for l in network.layers:\n",
    "        l.trainable = flag\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "inputs_b = Input(shape=(X_train.shape[1],))\n",
    "inputs_s = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "Dx = Dense(32, activation=\"relu\")(inputs)\n",
    "Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "Dx = Dense(32, activation=\"relu\")(Dx)\n",
    "Dx = Dense(1, activation=\"sigmoid\")(Dx)\n",
    "D = Model(input=[inputs], output=[Dx])\n",
    "\n",
    "#@TODO: Gradient reversal layer, and simul training\n",
    "#@TODO: loss on only the signal, we want that to be flat\n",
    "Rx = Dx\n",
    "Rx = Dense(32, activation=\"relu\")(Rx)\n",
    "Rx = Dense(32, activation=\"relu\")(Rx)\n",
    "Rx = Dense(32, activation=\"relu\")(Rx)\n",
    "#Rx = Dense(1, activation=\"sigmoid\")(Rx) #try regression activations @TODO\n",
    "#Rx = Dense(1, activation=\"relu\")(Rx)\n",
    "Rx = Dense(10, activation=\"softmax\")(Rx)\n",
    "R = Model(input=[inputs], outputs=[Rx])\n",
    "#@TODO: loss only on background events, tanh activation, batch norm, drop out, see Andreas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import mean_squared_error, categorical_crossentropy\n",
    "\n",
    "lam = 3.0 #10.0 # pivotal trade-off\n",
    "\n",
    "def make_loss_D(c):\n",
    "    def loss_D(y_true, y_pred):\n",
    "        #return c * K.binary_crossentropy(y_true, y_pred)\n",
    "        return c * K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) #!!! new keras from 2.0.7\n",
    "    return loss_D\n",
    "\n",
    "def make_loss_R(c):\n",
    "    def loss_R(z_true, z_pred):\n",
    "        #return c * mean_squared_error(z_true, z_pred) ##!!! new keras from 2.0.7\n",
    "        return c * categorical_crossentropy(z_true, z_pred)\n",
    "    return loss_R\n",
    "\n",
    "#opt_D = SGD()\n",
    "opt_D = \"adam\"\n",
    "D.compile(loss=[make_loss_D(c=1.0)], optimizer=opt_D)\n",
    "#D.compile(loss=\"binary_crossentropy\", optimizer=opt_D)\n",
    "\n",
    "# Train D such that R loss (its c=-lam) is also minimised, make it invariant to R\n",
    "#can we train simultaneous? grad reversal layer???\n",
    "#opt_DRf = SGD(momentum=0.0)\n",
    "opt_DRf = \"adam\"\n",
    "DRf = Model(input=[inputs], output=[D(inputs), R(inputs)])\n",
    "# R only on signal\n",
    "#DRf = Model(input=[inputs_b, inputs_s], output=[D(inputs_b),D(inputs_s), R(inputs_s)])\n",
    "make_trainable(R, False)\n",
    "make_trainable(D, True)\n",
    "# R only on signal\n",
    "#DRf.compile(loss=[make_loss_D(c=1.0),make_loss_D(c=1.0), make_loss_R(c=-lam)], optimizer=opt_DRf)\n",
    "DRf.compile(loss=[make_loss_D(c=1.0), make_loss_R(c=-lam)], optimizer=opt_DRf)\n",
    "#DRf.compile(loss=[\"binary_crossentropy\", \"mean_squared_error\"], loss_weights=[1,-lam], optimizer=opt_DRf)\n",
    "\n",
    "#opt_DfR = SGD(momentum=0.0)\n",
    "opt_DfR = \"adam\"\n",
    "DfR = Model(input=[inputs], output=[R(inputs)])\n",
    "make_trainable(R, True)\n",
    "make_trainable(D, False)\n",
    "#DfR.compile(loss=[make_loss_R(c=1.0)], optimizer=opt_DfR)\n",
    "DfR.compile(loss=[make_loss_R(c=lam)], optimizer=opt_DfR)\n",
    "#DfR.compile(loss=[\"mean_squared_error\"], loss_weights=[lam],optimizer=opt_DfR)\n",
    "#DfR.compile(loss=\"mean_squared_error\",optimizer=opt_DfR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "167568/167568 [==============================] - 14s - loss: 0.4151    \n",
      "Epoch 2/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.4058    \n",
      "Epoch 3/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.4035    \n",
      "Epoch 4/15\n",
      "167568/167568 [==============================] - 14s - loss: 0.4021    \n",
      "Epoch 5/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.4007    \n",
      "Epoch 6/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.4005    \n",
      "Epoch 7/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3988    \n",
      "Epoch 8/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3991    \n",
      "Epoch 9/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3984    \n",
      "Epoch 10/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3976    \n",
      "Epoch 11/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3977    \n",
      "Epoch 12/15\n",
      "167568/167568 [==============================] - 14s - loss: 0.3972    \n",
      "Epoch 13/15\n",
      "167568/167568 [==============================] - 14s - loss: 0.3964    \n",
      "Epoch 14/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3963    \n",
      "Epoch 15/15\n",
      "167568/167568 [==============================] - 13s - loss: 0.3959    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116376850>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrain D\n",
    "make_trainable(R, False)\n",
    "make_trainable(D, True)\n",
    "D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=15) # 15 epochs\n",
    "#D.fit(X_train, y_train, sample_weight=weights_train, nb_epoch=10) # 10 epochs\n",
    "\n",
    "#D.fit(X_train, y_train, nb_epoch=10, class_weight=get_class_weights(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp = DfR.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.shape, mass_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  1728/167568 [..............................] - ETA: 15s - loss: 3.5684"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167568/167568 [==============================] - 14s - loss: 3.5003    \n",
      "Epoch 2/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.4974    \n",
      "Epoch 3/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.4990    \n",
      "Epoch 4/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.5009    \n",
      "Epoch 5/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.5002    \n",
      "Epoch 6/15\n",
      "167568/167568 [==============================] - 13s - loss: 3.5019    \n",
      "Epoch 7/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.5025    \n",
      "Epoch 8/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.5000    \n",
      "Epoch 9/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.4962    \n",
      "Epoch 10/15\n",
      "167568/167568 [==============================] - 15s - loss: 3.4980    \n",
      "Epoch 11/15\n",
      "167568/167568 [==============================] - 13s - loss: 3.4992    \n",
      "Epoch 12/15\n",
      "167568/167568 [==============================] - 13s - loss: 3.4983    \n",
      "Epoch 13/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.5009    \n",
      "Epoch 14/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.5019    \n",
      "Epoch 15/15\n",
      "167568/167568 [==============================] - 14s - loss: 3.4993    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a32eb50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrain R\n",
    "make_trainable(R, True)\n",
    "make_trainable(D, False)\n",
    "# Train with noly signal\n",
    "#DfR.fit(X_train[y_train==1], mass_train[y_train==1], sample_weight=weights_train[y_train==1], nb_epoch=5) # only signal\n",
    "#DfR.fit([X_train[y_train==1], X_train[y_train==1]], [mass_train[y_train==1],mass_train[y_train==1]], sample_weight=[weights_train[y_train==0],weights_train[y_train==1]], nb_epoch=5) # only signal\n",
    "# Train with noly signal, mask with weights=0\n",
    "#DfR.fit(X_train, np.array(mass_train), sample_weight=weights_train*y_train, nb_epoch=5)\n",
    "DfR.fit(X_train, np.array(mass_train), sample_weight=weights_train*y_train, nb_epoch=15)\n",
    "#DfR.fit(X_train, mass_train, sample_weight=weights_train, nb_epoch=5)\n",
    "\n",
    "#DfR.fit(X_train, mass_train, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DfR.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DfR.evaluate(X_test, [mass_test], sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_losses(i, losses):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    ax1 = plt.subplot(311)   \n",
    "    values = np.array(losses[\"L_f\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f$\", color=\"blue\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax2 = plt.subplot(312, sharex=ax1) \n",
    "    values = np.array(losses[\"L_r\"]) #/ lam\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_r$\", color=\"green\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax3 = plt.subplot(313, sharex=ax1)\n",
    "    values = np.array(losses[\"L_f - L_r\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f - \\lambda L_r$\", color=\"red\")  \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "losses = {\"L_f\": [], \"L_r\": [], \"L_f - L_r\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 128\n",
    "training_iterations = 50#201\n",
    "for i in range(training_iterations):\n",
    "    l = DRf.evaluate(X_test, [y_test, mass_test], sample_weight=[weights_test,weights_test], verbose=0)\n",
    "    losses[\"L_f - L_r\"].append(l[0][None][0])\n",
    "    losses[\"L_f\"].append(l[1][None][0]) # why none, 0? just do l[1]??\n",
    "    losses[\"L_r\"].append(-l[2][None][0])\n",
    "    print(losses[\"L_f\"][-1], losses[\"L_r\"][-1] / lam, losses[\"L_r\"][-1])\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        plot_losses(i, losses)\n",
    "\n",
    "    # Fit D\n",
    "    make_trainable(R, False)\n",
    "    make_trainable(D, True)\n",
    "    indices = np.random.permutation(len(X_train))[:batch_size]\n",
    "    print \"DRf\"\n",
    "    DRf.train_on_batch(X_train[indices], [y_train[indices], mass_train[indices]], sample_weight=[weights_train[indices], weights_train[indices]])\n",
    "        \n",
    "    # Fit R\n",
    "    make_trainable(R, True)\n",
    "    make_trainable(D, False)\n",
    "    print \"DfR\"\n",
    "    DfR.fit(X_train, mass_train, batch_size=batch_size, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #@TODO: try grad reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87115890431426046"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_dc = D.predict(X_test)\n",
    "y_pred_dc = y_pred_dc.ravel()\n",
    "roc_auc_score(y_test, y_pred_dc, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(y_pred_dc[mass_test<mass_test.mean()], weights=weights_test[mass_test<mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred_dc[mass_test>=mass_test.mean()], weights=weights_test[mass_test>=mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    3\n",
       "2    1\n",
       "3    1\n",
       "4    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_test.idxmax(axis=1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted correlation with mass is (0.12821576397613105, 2.390160302825828e-203)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr = pearsonr(mass_test.idxmax(axis=1), y_pred_dc)\n",
    "#corr = pearsonr(mass_test, y_pred_dc)\n",
    "print \"Unweighted correlation with mass is\", corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40546122,  0.65210372,  0.09320176, ...,  0.23547377,\n",
       "        0.02123938,  0.05399346], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict = R.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.44230175e-01,   2.65018493e-01,   1.82857484e-01, ...,\n",
       "          2.85829487e-03,   6.70787820e-04,   7.84597287e-05],\n",
       "       [  3.35539281e-01,   2.66117513e-01,   1.85479656e-01, ...,\n",
       "          2.58310116e-03,   6.29214104e-04,   6.16722755e-05],\n",
       "       [  3.96219015e-01,   2.67207623e-01,   1.68884665e-01, ...,\n",
       "          2.28559086e-03,   3.70396388e-04,   4.82630676e-05],\n",
       "       ..., \n",
       "       [  3.49966884e-01,   2.64736921e-01,   1.81060687e-01, ...,\n",
       "          2.92390259e-03,   6.59864920e-04,   8.31508514e-05],\n",
       "       [  4.24777716e-01,   2.67906427e-01,   1.60329401e-01, ...,\n",
       "          1.87418563e-03,   2.42231690e-04,   3.07133705e-05],\n",
       "       [  4.12318766e-01,   2.67872125e-01,   1.64160520e-01, ...,\n",
       "          2.03122036e-03,   2.89392861e-04,   3.69389745e-05]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55424/55856 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0067945793764937834"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DfR.evaluate(X_test, [np.array(mass_test)], sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    2\n",
       " 1    3\n",
       " 2    1\n",
       " 3    1\n",
       " 4    2\n",
       " dtype: int64,    0  1  2  3  4  5  6  7  8  9\n",
       " 0  0  0  1  0  0  0  0  0  0  0\n",
       " 1  0  0  0  1  0  0  0  0  0  0\n",
       " 2  0  1  0  0  0  0  0  0  0  0\n",
       " 3  0  1  0  0  0  0  0  0  0  0\n",
       " 4  0  0  1  0  0  0  0  0  0  0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_test.idxmax(axis=1).head(5), mass_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict_cat = pd.DataFrame(mass_predict).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Series([], dtype: int64),\n",
       " array([[  3.44230175e-01,   2.65018493e-01,   1.82857484e-01,\n",
       "           1.08612917e-01,   5.99556789e-02,   2.62843240e-02,\n",
       "           9.43332072e-03,   2.85829487e-03,   6.70787820e-04,\n",
       "           7.84597287e-05],\n",
       "        [  3.35539281e-01,   2.66117513e-01,   1.85479656e-01,\n",
       "           1.11823678e-01,   6.21559285e-02,   2.66286954e-02,\n",
       "           8.98126047e-03,   2.58310116e-03,   6.29214104e-04,\n",
       "           6.16722755e-05],\n",
       "        [  3.96219015e-01,   2.67207623e-01,   1.68884665e-01,\n",
       "           8.98024589e-02,   4.92487662e-02,   1.88377518e-02,\n",
       "           7.09548406e-03,   2.28559086e-03,   3.70396388e-04,\n",
       "           4.82630676e-05],\n",
       "        [  3.63217711e-01,   2.65588284e-01,   1.77752778e-01,\n",
       "           1.01519883e-01,   5.62514588e-02,   2.35852879e-02,\n",
       "           8.68616719e-03,   2.75723240e-03,   5.68157178e-04,\n",
       "           7.31344626e-05],\n",
       "        [  3.57011408e-01,   2.65224278e-01,   1.79315343e-01,\n",
       "           1.03798844e-01,   5.75820804e-02,   2.45536882e-02,\n",
       "           8.99187289e-03,   2.83514196e-03,   6.09586481e-04,\n",
       "           7.77159366e-05]], dtype=float32),\n",
       "        0  1  2  3  4  5  6  7  8  9\n",
       " 1578   0  0  0  0  0  0  0  0  0  1\n",
       " 9921   0  0  0  0  0  0  0  0  0  1\n",
       " 15773  0  0  0  0  0  0  0  0  0  1\n",
       " 20988  0  0  0  0  0  0  0  0  0  1\n",
       " 35858  0  0  0  0  0  0  0  0  0  1\n",
       " 37977  0  0  0  0  0  0  0  0  0  1\n",
       " 52423  0  0  0  0  0  0  0  0  0  1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_predict_cat[mass_predict_cat>1], mass_predict[:5], mass_test[mass_test[9]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_predict_cat[mass_predict_cat>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([  3.96593839e-01,   2.67229527e-01,   1.68777093e-01,\n",
       "          8.96721855e-02,   4.91682813e-02,   1.87865626e-02,\n",
       "          7.07703782e-03,   2.27919035e-03,   3.68247362e-04,\n",
       "          4.79572445e-05], dtype=float32))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_predict_cat.iloc[1578], mass_predict[1578]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([  2.91798919e-01,   2.66274273e-01,   1.95233554e-01,\n",
       "          1.29758403e-01,   7.56870136e-02,   3.02116089e-02,\n",
       "          8.30481388e-03,   2.03262712e-03,   6.62873266e-04,\n",
       "          3.59449696e-05], dtype=float32))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_predict_cat.iloc[508], mass_predict[508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAIdAAJBoANAIP4fEbSveIAKHwkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1296a2b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.hist(mass_predict, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "#plt.hist(mass_test, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "plt.hist(mass_predict_cat/9, weights=weights_test, bins=10, histtype=\"step\", normed=1,range=(0,1), label=\"Predicted\")\n",
    "plt.hist(mass_test.idxmax(axis=1)/9, weights=weights_test, bins=10, histtype=\"step\", normed=1,range=(0,1), label=\"True\")\n",
    "\n",
    "\n",
    "\n",
    "#plt.ylim(0, 5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#doesnt change after adversrial training.. should get worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mass_predict[y_test==1], weights=weights_test[y_test==1], bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "plt.hist(mass_test[y_test==1], weights=weights_test[y_test==1], bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "\n",
    "#plt.ylim(plt.titlelt.title(\"VBF Events\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dc_train = D.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_pred_test_sig = [weights_train[(y_train ==1) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "#int_pred_test_bkg = [weights_train[(y_train ==0) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "int_pred_dc_test_sig = [weights_test[(y_test ==1) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_dc_test_bkg = [weights_test[(y_test ==0) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_sig)\n",
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import amsasimov\n",
    "vamsasimov_dc = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_dc_test_sig,int_pred_dc_test_bkg)]\n",
    "significance_dc = max(vamsasimov_dc)\n",
    "threshold_dc = np.linspace(0,1,num=50)[ np.array(vamsasimov_dc).argmax() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_dc, threshold_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import compare_train_test\n",
    "compare_train_test(y_pred_dc_train, y_train, y_pred_dc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGevMass(mass):\n",
    "    return np.exp (mass * (mass_max - mass_min) + mass_min)\n",
    "plt.hist(getGevMass(mass_test), weights=weights_test, bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score > \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Mass Distribution (s+b)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = np.zeros((len(mass_test),), dtype=[('mass',np.float64),('weight',np.float64),('NN_score',np.float64) ])\n",
    "temp['mass'] = np.array(getGevMass(mass_test))\n",
    "temp['weight'] = np.array(weights_test)\n",
    "temp['NN_score'] = np.array(y_pred_dc)\n",
    "\n",
    "from root_numpy import array2tree\n",
    "tree_dc = array2tree(temp)\n",
    "\n",
    "from ROOT import TEfficiency, TH1F\n",
    "bins = 50\n",
    "scoremin = temp['mass'].min()\n",
    "scoremax = temp['mass'].max()\n",
    "hpreselect_dc = TH1F(\"hpreselect_dc\", \"mass distribution before NN\", bins, scoremin, scoremax)\n",
    "hpreselect_dc.Sumw2()\n",
    "hD = TH1F(\"hD\", \"mass distribution for D Score > \" + str(threshold_dc), bins, scoremin, scoremax)\n",
    "hD.Sumw2()\n",
    "#tree.Project(\"hpreselect\", \"mass\", \"weight\" ) #Tefficiency can;t do weights\n",
    "tree_dc.Project(\"hpreselect_dc\", \"mass\" )\n",
    "#tree.Project(\"hNN\", \"mass\", \"weight*(NN_score>=\" +str(threshold) + \")\" )\n",
    "tree_dc.Project(\"hD\", \"mass\", \"(NN_score>=\" +str(threshold_dc) + \")\" )\n",
    "\n",
    "print TEfficiency.CheckConsistency(hD, hpreselect_dc)\n",
    "pEff_dc = TEfficiency(hD, hpreselect_dc)\n",
    "\n",
    "from ROOT import TCanvas\n",
    "c_dc = TCanvas(\"dcCanvas\",\"dc Canvas\",800,350)\n",
    "pEff_dc.SetTitle(\"Efficiency: Pre-selection vs Post D Selection;Mass (GeV) ;#epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pEff_dc.Draw(\"AP\")\n",
    "#ROOT.enableJSVis()\n",
    "c_dc.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mass_predict !=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again with adverserial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(i, losses):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    ax1 = plt.subplot(311)   \n",
    "    values = np.array(losses[\"L_f\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f$\", color=\"blue\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax2 = plt.subplot(312, sharex=ax1) \n",
    "    values = np.array(losses[\"L_r\"]) #/ lam\n",
    "    plt.plot(range(len(values)), values, label=r\"$\\lambda L_r$\", color=\"green\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax3 = plt.subplot(313, sharex=ax1)\n",
    "    values = np.array(losses[\"L_f - L_r\"])\n",
    "    plt.plot(range(len(values)), values, label=r\"$L_f - \\lambda L_r$\", color=\"red\")  \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"L_f\": [], \"L_r\": [], \"L_f - L_r\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "training_iterations = 201 #50\n",
    "for i in range(training_iterations):\n",
    "    #l = DRf.evaluate(X_test, [y_test, mass_test], sample_weight=[weights_test,weights_test], verbose=0)  \n",
    "    l = DRf.evaluate(X_train, [y_train, mass_train], sample_weight=[weights_train,weights_train], verbose=0)  \n",
    "    losses[\"L_f - L_r\"].append(l[0][None][0])\n",
    "    losses[\"L_f\"].append(l[1][None][0]) # why none, 0? just do l[1]??\n",
    "    losses[\"L_r\"].append(-l[2][None][0]) # the - cancels the - in loss -lam\n",
    "    print(losses[\"L_f\"][-1], losses[\"L_r\"][-1] / lam, losses[\"L_r\"][-1])\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        plot_losses(i, losses)\n",
    "\n",
    "    # Fit D\n",
    "    make_trainable(R, False)\n",
    "    make_trainable(D, True)\n",
    "    indices = np.random.permutation(len(X_train))[:batch_size]\n",
    "    print \"DRf\"\n",
    "    ##DRf.train_on_batch(X_train[indices], [y_train[indices], mass_train[indices]], sample_weight=[weights_train[indices], weights_train[indices]])\n",
    "    DRf.train_on_batch(X_train[indices], [y_train[indices], mass_train[indices]], sample_weight=[weights_train[indices], weights_train[indices]*y_train[indices]])\n",
    "    #@TODO: Make explicite masking with weights, y_train mught become multiclass later\n",
    "    \n",
    "    # Fit R\n",
    "    make_trainable(R, True)\n",
    "    make_trainable(D, False)\n",
    "    print \"DfR\"\n",
    "    #masking background events\n",
    "    DfR.fit(X_train, mass_train, batch_size=batch_size, sample_weight=weights_train*y_train, nb_epoch=1, verbose=1)\n",
    "    ##DfR.fit(X_train, mass_train, batch_size=batch_size, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, nb_epoch=1, verbose=1)\n",
    "    #DfR.fit(X_train, mass_train, sample_weight=weights_train, nb_epoch=1, verbose=1)\n",
    "    #@TODO: try grad reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_dc = D.predict(X_test)\n",
    "y_pred_dc = y_pred_dc.ravel()\n",
    "roc_auc_score(y_test, y_pred_dc, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(y_pred_dc[mass_test<mass_test.mean()], weights=weights_test[mass_test<mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"Low\")\n",
    "plt.hist(y_pred_dc[mass_test>=mass_test.mean()], weights=weights_test[mass_test>=mass_test.mean()], bins=50, histtype=\"step\", normed=1, label=\"High\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr = pearsonr(mass_test, y_pred_dc)\n",
    "print \"Unweighted correlation with mass is\", corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict = R.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DfR.evaluate(X_test, [mass_test], sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mass_predict, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"Predicted\")\n",
    "plt.hist(mass_test, weights=weights_test, bins=50, histtype=\"step\", normed=1, label=\"True\")\n",
    "\n",
    "#plt.ylim(0, 5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#doesnt change after adversrial training.. should get worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dc_train = D.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_pred_test_sig = [weights_train[(y_train ==1) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "#int_pred_test_bkg = [weights_train[(y_train ==0) & (y_pred_train > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "int_pred_dc_test_sig = [weights_test[(y_test ==1) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_dc_test_bkg = [weights_test[(y_test ==0) & (y_pred_dc > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_sig)\n",
    "plt.plot(np.linspace(0,1,num=50),int_pred_dc_test_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import amsasimov\n",
    "vamsasimov_dc = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_dc_test_sig,int_pred_dc_test_bkg)]\n",
    "significance_dc = max(vamsasimov_dc)\n",
    "threshold_dc = np.linspace(0,1,num=50)[ np.array(vamsasimov_dc).argmax() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_dc, threshold_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_func import compare_train_test\n",
    "compare_train_test(y_pred_dc_train, y_train, y_pred_dc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGevMass(mass):\n",
    "    return np.exp (mass * (mass_max - mass_min) + mass_min)\n",
    "plt.hist(getGevMass(mass_test), weights=weights_test, bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Preselection\")\n",
    "plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Mass Distribution (s+b)\")\n",
    "#plt.ylim(0, 4)\n",
    "plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But R looks at all events, not just the ones with high score so it tries to \n",
    "#also predict the mass of background. It shouldnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test,y=y_pred_dc, weights=weights_test, bins=50, label=\"Preselection\")\n",
    "#plt.hist2d(x=getGevMass(mass_test),y=y_pred_dc, weights=weights_test, bins=50, normed=1, label=\"Preselection\")\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of all Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test[y_pred_dc >= threshold_dc],y=y_pred_dc[y_pred_dc >= threshold_dc], weights=weights_test[y_pred_dc >= threshold_dc], bins=50, label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of Score >= \" + str(threshold_dc)+\" Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test[y_test ==1],y=y_pred_dc[y_test ==1], weights=weights_test[y_test ==1], bins=50, label=\"True VBF Events\")\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of Score >= \" + str(threshold_dc)+\" Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(x=mass_test[y_pred_dc < threshold_dc],y=y_pred_dc[y_pred_dc < threshold_dc], weights=weights_test[y_pred_dc < threshold_dc], bins=50, label=\"Score < \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc >= threshold_dc]), weights=weights_test[y_pred_dc >= threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score >= \" + str(threshold_dc))\n",
    "#plt.hist(getGevMass(mass_test[y_pred_dc < threshold_dc]), weights=weights_test[y_pred_dc < threshold_dc], bins=50, histtype=\"step\", normed=1, range=(220, 2000), label=\"Score < \" + str(threshold_dc))\n",
    "\n",
    "plt.title(\"Score vs Mass of Score < \" + str(threshold_dc)+\" Events\")\n",
    "#plt.ylim(0, 4)\n",
    "#plt.xlim(200, 1200)\n",
    "plt.xlabel(\"mass GeV\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((len(mass_test),), dtype=[('mass',np.float64),('weight',np.float64),('NN_score',np.float64) ])\n",
    "temp['mass'] = np.array(getGevMass(mass_test))\n",
    "temp['weight'] = np.array(weights_test)\n",
    "temp['NN_score'] = np.array(y_pred_dc)\n",
    "\n",
    "from root_numpy import array2tree\n",
    "tree_dc = array2tree(temp)\n",
    "\n",
    "from ROOT import TEfficiency, TH1F\n",
    "bins = 50\n",
    "scoremin = temp['mass'].min()\n",
    "scoremax = temp['mass'].max()\n",
    "hpreselect_dc = TH1F(\"hpreselect_dc\", \"mass distribution before NN\", bins, scoremin, scoremax)\n",
    "hpreselect_dc.Sumw2()\n",
    "hD = TH1F(\"hD\", \"mass distribution for D Score > \" + str(threshold_dc), bins, scoremin, scoremax)\n",
    "hD.Sumw2()\n",
    "#tree.Project(\"hpreselect\", \"mass\", \"weight\" ) #Tefficiency can;t do weights\n",
    "tree_dc.Project(\"hpreselect_dc\", \"mass\" )\n",
    "#tree.Project(\"hNN\", \"mass\", \"weight*(NN_score>=\" +str(threshold) + \")\" )\n",
    "tree_dc.Project(\"hD\", \"mass\", \"(NN_score>=\" +str(threshold_dc) + \")\" )\n",
    "\n",
    "print TEfficiency.CheckConsistency(hD, hpreselect_dc)\n",
    "pEff_dc = TEfficiency(hD, hpreselect_dc)\n",
    "\n",
    "from ROOT import TCanvas\n",
    "c_dc = TCanvas(\"dcCanvas\",\"dc Canvas\",800,350)\n",
    "pEff_dc.SetTitle(\"Efficiency: Pre-selection vs Post D Selection;Mass (GeV) ;#epsilon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pEff_dc.Draw(\"AP\")\n",
    "#ROOT.enableJSVis()\n",
    "c_dc.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mass_predict !=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @NOW: Try multiclass bin 'regression' instead\n",
    "\n",
    "\n",
    "#: inject mass correlation and see if R learns anything at all. Find the elephant\n",
    "# R is sometimes learning somtimes not.. why.. reproducible problem?\n",
    "# the D without adverserial training is asmost same as after adverserial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# why is the loss look decreasing further on pivot training, its not actually cuz auc is worse\n",
    "#@TODO: Compare AUC/significance with qq, without qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Victor:\n",
    "# pivot not working well\n",
    "# V: Try to overfit on 100 samples: find elephant problems\n",
    "# regression activation, loss function?\n",
    "# V: Its okay trial and error | andreas did classification\n",
    "# loss of r is changing at a lower level, maybe subtract off the first loss?\n",
    "# V: maybe\n",
    "# grad reversal layer\n",
    "# V: simultaneous could be better\n",
    "# why is the loss look decreasing further on pivot training, its not actually cuz auc is worse\n",
    "# V: Maybe you need to let pre-training converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip2 install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ams loss function: celver way to impliment, if else wont work: like makeing soft cuts\n",
    "# how close to 0, how close to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "604px",
    "left": "0px",
    "right": "1070px",
    "top": "110px",
    "width": "128px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
